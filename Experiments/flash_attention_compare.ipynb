{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to for 4-bit model generation\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# import torch\n",
    "# import time\n",
    "\n",
    "# import bitsandbytes as bnb\n",
    "# print(bnb.__version__)\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# # 4-bit quantization config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n",
    "\n",
    "# # Load tokenizer + model from Hugging Face in 4-bit\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"cuda\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# # Test prompt\n",
    "# prompt = \"Explain quantum computing in simple terms.\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Generate output\n",
    "# with torch.no_grad():\n",
    "#     output_ids = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=100,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9\n",
    "#     )\n",
    "\n",
    "# print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to the model\n",
    "\n",
    "# save_path = \"./mistral_7b_4bit_local\"\n",
    "# model.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/root/miniconda/envs/likhitenv/lib/python3.10/site-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model from local directory\n",
    "local_model_path = \"./mistral_7b_4bit_local\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 2: HECK | Δt = 0.0625s\n",
      "Token 3: Y | Δt = 0.0620s\n",
      "Token 4: ES | Δt = 0.0608s\n",
      "Token 5: ! | Δt = 0.0603s\n",
      "Token 6: \n",
      " | Δt = 0.0615s\n",
      "Token 7: \n",
      " | Δt = 0.0604s\n",
      "Token 8: I | Δt = 0.0581s\n",
      "Token 9: ’ | Δt = 0.0582s\n",
      "Token 10: m | Δt = 0.0573s\n",
      "Token 11: not | Δt = 0.0568s\n",
      "Token 12: sure | Δt = 0.0566s\n",
      "Token 13: if | Δt = 0.0580s\n",
      "Token 14: I | Δt = 0.0586s\n",
      "Token 15: ’ | Δt = 0.0582s\n",
      "Token 16: m | Δt = 0.0581s\n",
      "Token 17: more | Δt = 0.0581s\n",
      "Token 18: excited | Δt = 0.0585s\n",
      "Token 19: about | Δt = 0.0583s\n",
      "Token 20: the | Δt = 0.0585s\n",
      "Token 21: fact | Δt = 0.0567s\n",
      "Token 22: that | Δt = 0.0573s\n",
      "Token 23: I | Δt = 0.0571s\n",
      "Token 24: ’ | Δt = 0.0566s\n",
      "Token 25: m | Δt = 0.0584s\n",
      "Token 26: going | Δt = 0.0583s\n",
      "Token 27: to | Δt = 0.0576s\n",
      "Token 28: be | Δt = 0.0565s\n",
      "Token 29: able | Δt = 0.0640s\n",
      "Token 30: to | Δt = 0.0607s\n",
      "Token 31: see | Δt = 0.0569s\n",
      "Token 32: the | Δt = 0.0571s\n",
      "Token 33: new | Δt = 0.0564s\n",
      "Token 34: Star | Δt = 0.0570s\n",
      "Token 35: Wars | Δt = 0.0567s\n",
      "Token 36: movie | Δt = 0.0565s\n",
      "Token 37: in | Δt = 0.0565s\n",
      "Token 38: the | Δt = 0.0574s\n",
      "Token 39: aters | Δt = 0.0572s\n",
      "Token 40: or | Δt = 0.0566s\n",
      "Token 41: that | Δt = 0.0565s\n",
      "Token 42: I | Δt = 0.0574s\n",
      "Token 43: ’ | Δt = 0.0572s\n",
      "Token 44: m | Δt = 0.0565s\n",
      "Token 45: going | Δt = 0.0566s\n",
      "Token 46: to | Δt = 0.0573s\n",
      "Token 47: be | Δt = 0.0583s\n",
      "Token 48: able | Δt = 0.0586s\n",
      "Token 49: to | Δt = 0.0581s\n",
      "Token 50: see | Δt = 0.0571s\n",
      "Token 51: it | Δt = 0.0569s\n",
      "Token 52: in | Δt = 0.0567s\n",
      "Token 53:  | Δt = 0.0565s\n",
      "Token 54: 3 | Δt = 0.0574s\n",
      "Token 55: D | Δt = 0.0567s\n",
      "Token 56: . | Δt = 0.0564s\n",
      "Token 57: \n",
      " | Δt = 0.0564s\n",
      "Token 58: \n",
      " | Δt = 0.0571s\n",
      "Token 59: I | Δt = 0.0569s\n",
      "Token 60: ’ | Δt = 0.0568s\n",
      "Token 61: m | Δt = 0.0562s\n",
      "Token 62: a | Δt = 0.0570s\n",
      "Token 63: huge | Δt = 0.0569s\n",
      "Token 64: Star | Δt = 0.0564s\n",
      "Token 65: Wars | Δt = 0.0565s\n",
      "Token 66: fan | Δt = 0.0579s\n",
      "Token 67: . | Δt = 0.0657s\n",
      "Token 68: I | Δt = 0.0574s\n",
      "Token 69: ’ | Δt = 0.0565s\n",
      "Token 70: ve | Δt = 0.0572s\n",
      "Token 71: seen | Δt = 0.0576s\n",
      "Token 72: all | Δt = 0.0567s\n",
      "Token 73: the | Δt = 0.0566s\n",
      "Token 74: movies | Δt = 0.0576s\n",
      "Token 75: multiple | Δt = 0.0567s\n",
      "Token 76: times | Δt = 0.0565s\n",
      "Token 77: . | Δt = 0.0566s\n",
      "Token 78: I | Δt = 0.0570s\n",
      "Token 79: ’ | Δt = 0.0570s\n",
      "Token 80: ve | Δt = 0.0566s\n",
      "Token 81: read | Δt = 0.0570s\n",
      "Token 82: all | Δt = 0.0572s\n",
      "Token 83: the | Δt = 0.0568s\n",
      "Token 84: books | Δt = 0.0565s\n",
      "Token 85: . | Δt = 0.0566s\n",
      "Token 86: I | Δt = 0.0579s\n",
      "Token 87: ’ | Δt = 0.0587s\n",
      "Token 88: ve | Δt = 0.0580s\n",
      "Token 89: played | Δt = 0.0581s\n",
      "Token 90: all | Δt = 0.0585s\n",
      "Token 91: the | Δt = 0.0582s\n",
      "Token 92: video | Δt = 0.0565s\n",
      "Token 93: games | Δt = 0.0565s\n",
      "Token 94: . | Δt = 0.0572s\n",
      "Token 95: I | Δt = 0.0575s\n",
      "Token 96: ’ | Δt = 0.0565s\n",
      "Token 97: ve | Δt = 0.0566s\n",
      "Token 98: even | Δt = 0.0573s\n",
      "Token 99: dressed | Δt = 0.0567s\n",
      "Token 100: up | Δt = 0.0565s\n",
      "\n",
      "===== PERFORMANCE METRICS =====\n",
      "Prefill time: 0.3780 s\n",
      "Total decode time: 5.7683 s\n",
      "Time to first token (TTFT): 0.0628 s\n",
      "Average time between tokens (TBT): 0.0576 s\n",
      "Generated text:\n",
      "What is reinforcement learning?\n",
      "HECK YES!\n",
      "\n",
      "I’m not sure if I’m more excited about the fact that I’m going to be able to see the new Star Wars movie in theaters or that I’m going to be able to see it in 3D.\n",
      "\n",
      "I’m a huge Star Wars fan. I’ve seen all the movies multiple times. I’ve read all the books. I’ve played all the video games. I’ve even dressed up\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"What is reinforcement learning?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ------------------------------\n",
    "# Measure Prefill + Decode\n",
    "# ------------------------------\n",
    "max_new_tokens = 100\n",
    "\n",
    "# 1️⃣ Prefill: forward pass on input prompt (no generation)\n",
    "torch.cuda.synchronize()\n",
    "start_prefill = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs, use_cache=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_prefill = time.time()\n",
    "prefill_time = end_prefill - start_prefill\n",
    "\n",
    "# 2️⃣ Decode: generate tokens and measure TTFT and TBT\n",
    "generated = inputs[\"input_ids\"]\n",
    "past_key_values = None\n",
    "new_tokens = []\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_decode = time.time()\n",
    "first_token_time = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "        # Run one forward step\n",
    "        outputs = model(input_ids=generated[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Sample next token\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "        new_tokens.append(next_token.item())\n",
    "\n",
    "        # Measure TTFT and per-token latency\n",
    "        torch.cuda.synchronize()\n",
    "        now = time.time()\n",
    "\n",
    "        if i == 0:\n",
    "            ttft = now - start_decode  # Time to first token\n",
    "            last_token_time = now\n",
    "        else:\n",
    "            tbt = now - last_token_time\n",
    "            last_token_time = now\n",
    "            print(f\"Token {i+1}: {tokenizer.decode(next_token[0])} | Δt = {tbt:.4f}s\")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_decode = time.time()\n",
    "\n",
    "decode_time = end_decode - start_decode\n",
    "avg_tbt = (decode_time - ttft) / (len(new_tokens) - 1) if len(new_tokens) > 1 else 0\n",
    "\n",
    "# ------------------------------\n",
    "# Results\n",
    "# ------------------------------\n",
    "print(\"\\n===== PERFORMANCE METRICS =====\")\n",
    "print(f\"Prefill time: {prefill_time:.4f} s\")\n",
    "print(f\"Total decode time: {decode_time:.4f} s\")\n",
    "print(f\"Time to first token (TTFT): {ttft:.4f} s\")\n",
    "print(f\"Average time between tokens (TBT): {avg_tbt:.4f} s\")\n",
    "print(f\"Generated text:\\n{tokenizer.decode(generated[0], skip_special_tokens=True)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
