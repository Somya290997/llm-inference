{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likhit/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-0425-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"What is Machine Learning is the best that humans have ever bulit  in there lives and tell us now check out of this really work can you  help me to start ?\"\n",
    "# # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# inputs = tokenizer(input , return_tensors=\"pt\")\n",
    "# # model\n",
    "# print(inputs[\"input_ids\"].shape)\n",
    "# print(inputs[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_hidden_states = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "# print(st_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_global = model.model.\n",
    "# pos_local = model.model.rotary_emb_local(st_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i , layer in enumerate(model.model.embed_tokens):\n",
    "#     print(i)\n",
    "#     print(layer)\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.olmo2.modeling_olmo2 import Olmo2RotaryEmbedding\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 1. Embed the prompt\n",
    "# # ------------------------------------------------------------------\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "# hidden = model.model.embed_tokens(input_ids)                 # (1, L, 2048)\n",
    "\n",
    "# seq_len = input_ids.size(1)\n",
    "# position_ids = torch.arange(seq_len, device=device).unsqueeze(0)   # (1, L)\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 2. Build a *causal* attention mask (exactly what HF does)\n",
    "# # ------------------------------------------------------------------\n",
    "# attn_mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=device)\n",
    "# attn_mask = torch.triu(attn_mask, diagonal=1)               # upper triangle = -inf\n",
    "# attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)             # (1,1,L,L)\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 3. Pre-compute rotary cos/sin for the whole prompt\n",
    "# # ------------------------------------------------------------------\n",
    "# rotary_emb: Olmo2RotaryEmbedding = model.model.rotary_emb\n",
    "# cos, sin = rotary_emb(position_ids)                         # (1, L, 1, head_dim)\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 4. KV-cache container (list of (k,v) per layer)\n",
    "# # ------------------------------------------------------------------\n",
    "# past_key_values = [None] * len(model.model.layers)\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 5. Loop over layers\n",
    "# # ------------------------------------------------------------------\n",
    "# for i, layer in enumerate(model.model.layers):\n",
    "#     # The *exact* signature used inside the full model:\n",
    "#     layer_out = layer(\n",
    "#         hidden_states=hidden,\n",
    "#         attention_mask=attn_mask,\n",
    "#         position_ids=position_ids,\n",
    "#         past_key_value=past_key_values[i],\n",
    "#         use_cache=True,\n",
    "#         cos=cos,                # <-- manual rotary\n",
    "#         sin=sin,                # <-- manual rotary\n",
    "#         output_attentions=False,\n",
    "#     )\n",
    "\n",
    "#     hidden = layer_out[0]                     # next hidden state\n",
    "#     past_key_values[i] = layer_out.past_key_value\n",
    "\n",
    "#     k, v = layer_out.past_key_value\n",
    "#     print(f\"[Layer {i:02d}] hidden={hidden.shape}  K={k.shape}  V={v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"What is Machine Learning is the best that humans have ever bulit  in there lives and tell us now check out of this really work can you  help me to start ?\"\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# inputs = tokenizer(prompt , return_tensors=\"pt\")\n",
    "# model\n",
    "# print(inputs[\"input_ids\"].shape)\n",
    "# print(inputs[\"attention_mask\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fun(i):\n",
    "#     print(f\"The layer\", i )\n",
    "    \n",
    "# for i , layers in enumerate(model.model.layers):\n",
    "#     layers.self_attn.register_forward_hook(fun(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook(layer_idx):\n",
    "#     def f(module, input, output):\n",
    "#         attn_out = output[0]\n",
    "#         print(output[1])\n",
    "#         print(f\"[Layer {layer_idx}] attention output: {attn_out.shape}\")\n",
    "#     return f\n",
    "\n",
    "# for i, layer in enumerate(model.model.layers):\n",
    "#     layer.self_attn.register_forward_hook(hook(i))\n",
    "\n",
    "# _ = model(**inputs, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total layers hooked:\", len(model.model.layers))\n",
    "\n",
    "# fired = []\n",
    "# def record_fired(layer_idx):\n",
    "#     def hook(module, input, output):\n",
    "#         fired.append(layer_idx)\n",
    "#         print(f\"[Hook fired] layer {layer_idx}\")\n",
    "#     return hook\n",
    "\n",
    "# for i, layer in enumerate(model.model.layers):\n",
    "#     layer.self_attn.register_forward_hook(record_fired(i))\n",
    "\n",
    "# try:\n",
    "#     _ = model(**inputs, use_cache=True)\n",
    "# except Exception as e:\n",
    "#     print(\"Error:\", e)\n",
    "\n",
    "# print(\"Layers that actually ran:\", fired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     out = model(**inputs, use_cache=True)\n",
    "\n",
    "# print(type(out.past_key_values))\n",
    "# print(len(out.past_key_values))\n",
    "\n",
    "# # one layer\n",
    "# k0, v0 = out.past_key_values[0]\n",
    "# print(k0.shape, v0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 10\n",
      "\n",
      "Starting layer-by-layer computation...\n",
      "\n",
      "✓ Layer 0: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 1: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 2: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 3: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 4: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 5: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 6: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 7: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 8: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 9: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 10: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 11: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 12: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 13: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 14: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 15: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 16: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "✓ Layer 17: Captured KV during computation - Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "\n",
      "==================================================\n",
      "✅ Successfully captured 18 layers during computation!\n",
      "==================================================\n",
      "Computation order: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "\n",
      "layer_0: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_1: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_2: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_3: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_4: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_5: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_6: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_7: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_8: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_9: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_10: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_11: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_12: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_13: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_14: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_15: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_16: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "layer_17: Key torch.Size([1, 1, 10, 256]), Value torch.Size([1, 1, 10, 256])\n",
      "\n",
      "✅ Each layer's KV cache was captured DURING forward pass, not after!\n"
     ]
    }
   ],
   "source": [
    "from transformers.cache_utils import DynamicCache\n",
    "kv_cache_storage = {}\n",
    "layer_computation_order = []\n",
    "\n",
    "# Create a custom cache class that intercepts updates\n",
    "class MonitoredCache(DynamicCache):\n",
    "    \"\"\"Custom cache that captures KV pairs as they're stored\"\"\"\n",
    "    \n",
    "    def update(self, key_states, value_states, layer_idx, cache_kwargs=None):\n",
    "        \"\"\"Called every time a layer stores its KV cache\"\"\"\n",
    "        \n",
    "        # Capture the cache immediately when it's stored\n",
    "        kv_cache_storage[f\"layer_{layer_idx}\"] = {\n",
    "            \"key\": key_states.clone().cpu(),\n",
    "            \"value\": value_states.clone().cpu()\n",
    "        }\n",
    "        layer_computation_order.append(layer_idx)\n",
    "        print(f\"✓ Layer {layer_idx}: Captured KV during computation - Key {key_states.shape}, Value {value_states.shape}\")\n",
    "        \n",
    "        # Call parent's update to maintain normal behavior\n",
    "        return super().update(key_states, value_states, layer_idx, cache_kwargs)\n",
    "\n",
    "# Run prefill with our custom cache\n",
    "prompt = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input tokens: {inputs['input_ids'].shape[1]}\\n\")\n",
    "print(\"Starting layer-by-layer computation...\\n\")\n",
    "\n",
    "# Create our monitored cache\n",
    "past_key_values = MonitoredCache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        past_key_values=past_key_values,  # Use our custom cache\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"✅ Successfully captured {len(kv_cache_storage)} layers during computation!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Computation order: {layer_computation_order}\")\n",
    "print()\n",
    "\n",
    "# Verify the captured caches\n",
    "for layer_name in sorted(kv_cache_storage.keys(), key=lambda x: int(x.split('_')[1])):\n",
    "    cache = kv_cache_storage[layer_name]\n",
    "    print(f\"{layer_name}: Key {cache['key'].shape}, Value {cache['value'].shape}\")\n",
    "\n",
    "# Optional: Save to disk\n",
    "# torch.save(kv_cache_storage, \"olmo2_kv_cache_layer_by_layer.pt\")\n",
    "\n",
    "print(f\"\\n✅ Each layer's KV cache was captured DURING forward pass, not after!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': tensor([[[[ 0.0054, -0.0421,  0.0082,  ..., -2.5615,  0.7074, -3.5098],\n",
       "           [-0.3547,  0.2818,  1.6888,  ..., -2.7601,  1.0438, -2.9864],\n",
       "           [-2.2232,  0.7170, -0.5147,  ...,  1.3554,  0.7316, -3.4654],\n",
       "           ...,\n",
       "           [ 0.0380, -0.6619,  1.1123,  ..., -3.2491,  0.7625, -2.7972],\n",
       "           [-1.7064,  0.2602,  0.4606,  ..., -0.2007,  1.9049, -4.6290],\n",
       "           [-1.5021,  1.3075, -1.2664,  ...,  3.7362,  2.7996, -1.8984]]]]),\n",
       " 'value': tensor([[[[ -0.8908,   0.2031,  -0.7874,  ...,   0.3974,   0.7160,   0.3788],\n",
       "           [  9.4710,   5.7023,   3.7420,  ...,   0.5967,  -1.1231,  -1.3172],\n",
       "           [ 12.3833, -20.9469,  -8.2705,  ..., -11.3474,  -4.3166,  12.1494],\n",
       "           ...,\n",
       "           [  8.6267,   2.7562,  -1.7541,  ...,  -1.4571,   1.5015,   1.7500],\n",
       "           [ 11.9342, -14.2396,   0.2285,  ...,  -1.9927,  -0.0937,   8.7626],\n",
       "           [ -8.1127,   6.9305,  -4.1905,  ..., -16.4426,  -1.0108,   1.0505]]]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache_storage['layer_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import inspect\n",
    "# from transformers.cache_utils import DynamicCache\n",
    "# print(inspect.getsource(DynamicCache.update))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
