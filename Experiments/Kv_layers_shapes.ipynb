{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likhit/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36])\n",
      "torch.Size([1, 36])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is Machine Learning is the best that humans have ever bulit  in there lives and tell us now check out of this really work can you  help me to start ?\"\n",
    "inputs = tokenizer(prompt , return_tensors=\"pt\").to(device)\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(inputs[\"attention_mask\"].shape)\n",
    "\n",
    "output = model(**inputs, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 262144])\n",
      "torch.Size([1, 262144])\n",
      "torch.Size([1, 262144])\n"
     ]
    }
   ],
   "source": [
    "# checking the next token\n",
    "print(output.logits.shape)\n",
    "last_token_logits = output.logits[:,-1,:]\n",
    "print(last_token_logits.shape)\n",
    "next_token_id = torch.softmax(last_token_logits,dim=-1)\n",
    "print(next_token_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer])\n",
      "torch.Size([1, 1, 36, 256])\n",
      "torch.Size([1, 1, 36, 256])\n",
      "torch.float32 torch.float32\n",
      "36.864 KB\n"
     ]
    }
   ],
   "source": [
    "# KV key_values stored in it.\n",
    "past_key_values = output.past_key_values\n",
    "print((past_key_values))\n",
    "k , v = output.past_key_values[0]\n",
    "print(k.shape)\n",
    "print(v.shape)\n",
    "print(k.dtype,v.dtype)\n",
    "\n",
    "size = 1 * 1 * 36 * 256 * 4\n",
    "print(size/1000, \"KB\")\n",
    "# Batch_size, n_head , seq_len , dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  next word predictions.\n",
    "logits_val , logis_idx = torch.topk(next_token_id,10,dim=-1)\n",
    "\n",
    "print(logits_val.shape)\n",
    "print(logis_idx)\n",
    "top_k_probs = logits_val / torch.sum(logits_val,dim=-1)\n",
    "print(top_k_probs.shape)\n",
    "probs = torch.multinomial(top_k_probs,1)\n",
    "print(probs.shape)\n",
    "print(probs)\n",
    "next_token_id = torch.gather(logis_idx, dim=-1, index=probs)\n",
    "print(next_token_id)\n",
    "\n",
    "# last token is \n",
    "word = tokenizer.decode(next_token_id[0])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he new k v values get attached to this KV cahce ) past_key_values and we get the logits of the output_next .. \n",
    "output_next = model(input_ids=next_token_id,past_key_values=past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP TOKEN: <eos>\n",
      "\n",
      "Joined prompt:\n",
      "What is machine Learning?<eos>Tell me a joke about cats.<eos>Who are you?\n",
      "Length of input ids:  18\n",
      "Separator ID: [1]\n",
      "Seq Len of sep: 1\n",
      "Token ranges: [(0, 5), (6, 13), (14, 18)]\n",
      "Seq Len: 18\n",
      "Final attn_mask shape: torch.Size([1, 4, 18, 18])\n",
      "Success — prompts packed WITHOUT cross-attending!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cpu\"   # or \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "prompts = [\n",
    "    \"What is machine Learning?\",\n",
    "    \"Tell me a joke about cats.\",\n",
    "    \"Who are you?\"\n",
    "]\n",
    "\n",
    "sep = tokenizer.eos_token  # SAFE separator\n",
    "print(\"SEP TOKEN:\", sep)\n",
    "\n",
    "# 1) Join prompts\n",
    "joined = sep.join(prompts)\n",
    "print(\"\\nJoined prompt:\")\n",
    "print(joined)\n",
    "\n",
    "tokens = tokenizer(joined, return_tensors=\"pt\",add_special_tokens=False).to(device)\n",
    "print( \"Length of input ids: \", len(tokens[\"input_ids\"][0]))\n",
    "\n",
    "# 2. Track each prompt's token range\n",
    "\n",
    "prompt_ranges = []\n",
    "cur = 0\n",
    "\n",
    "sep_id = tokenizer(sep, add_special_tokens=False)\n",
    "print(\"Separator ID:\", sep_id[\"input_ids\"])\n",
    "\n",
    "sep_len = sep_id[\"input_ids\"][0]\n",
    "print(\"Seq Len of sep:\", sep_len)\n",
    "\n",
    "for p in prompts:\n",
    "    tok_len = len(tokenizer(p,add_special_tokens=False)[\"input_ids\"])\n",
    "    prompt_ranges.append((cur, cur + tok_len))\n",
    "    cur += tok_len + sep_len\n",
    "\n",
    "print(\"Token ranges:\", prompt_ranges)\n",
    "\n",
    "# 3. Build block diagonal attention mask\n",
    "seq_len = tokens[\"input_ids\"].shape[1]\n",
    "\n",
    "print(\"Seq Len:\", seq_len)\n",
    "\n",
    "attn_mask = torch.zeros((seq_len, seq_len), dtype=torch.float, device=device)\n",
    "\n",
    "for (start, end) in prompt_ranges:\n",
    "    attn_mask[start:end, start:end] = 1\n",
    "\n",
    "# 4. Expand to correct shape: [batch, heads, seq_len, seq_len]\n",
    "num_heads = model.config.num_attention_heads\n",
    "attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)   # [1,1,seq,seq]\n",
    "attn_mask = attn_mask.repeat(1, num_heads, 1, 1)  # [1,h,seq,seq]\n",
    "\n",
    "print(\"Final attn_mask shape:\", attn_mask.shape)\n",
    "\n",
    "# 5. Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=tokens[\"input_ids\"],\n",
    "        attention_mask=attn_mask,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(\"Success — prompts packed WITHOUT cross-attending!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt1 token length = 5\n",
      "Sample k shape: torch.Size([1, 1, 5, 256])\n",
      "Sample v shape: torch.Size([1, 1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Extract KV cache for first prompt\n",
    "past_kv = outputs.past_key_values\n",
    "first_prompt_len = len(tokenizer(prompts[0], add_special_tokens=False)[\"input_ids\"])\n",
    "print(f\"Prompt1 token length = {first_prompt_len}\")\n",
    "\n",
    "# ✅ CORRECT: Don't stack - just slice each layer's tuple\n",
    "past = []\n",
    "for layer_kv in past_kv:\n",
    "    k, v = layer_kv  # Each is [batch, heads, seq, head_dim]\n",
    "    # Slice to keep only first prompt's tokens\n",
    "    k_sliced = k[:, :, 0:first_prompt_len, :]\n",
    "    v_sliced = v[:, :, 0:first_prompt_len, :]\n",
    "    past.append((k_sliced, v_sliced))\n",
    "\n",
    "print(\"Sample k shape:\", past[0][0].shape)  # Should be [1, 4, 6, 256]\n",
    "print(\"Sample v shape:\", past[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix text: 'What is machine Learning?'\n",
      "Prefix tokens: [3689, 563, 5464, 19180, 236881]\n",
      "Without cache: What is machine Learning???????????????????????????????\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the prefix actually is\n",
    "prefix_text = tokenizer.decode(tokens[\"input_ids\"][0, :first_prompt_len])\n",
    "print(\"Prefix text:\", repr(prefix_text))\n",
    "print(\"Prefix tokens:\", tokens[\"input_ids\"][0, :first_prompt_len].tolist())\n",
    "\n",
    "# Let's try without cache to see what SHOULD be generated\n",
    "with torch.no_grad():\n",
    "    out_no_cache = model.generate(\n",
    "        tokens[\"input_ids\"][:, :first_prompt_len],\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "    )\n",
    "print(\"Without cache:\", tokenizer.decode(out_no_cache[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output cache info:\n",
      "  Number of layers: 18\n",
      "  Layer 0 K shape: torch.Size([1, 1, 18, 256])\n",
      "  Layer 0 V shape: torch.Size([1, 1, 18, 256])\n",
      "\n",
      "Extracted cache info:\n",
      "  Number of layers: 18\n",
      "  Layer 0 K shape: torch.Size([1, 1, 5, 256])\n",
      "  Layer 0 V shape: torch.Size([1, 1, 5, 256])\n",
      "\n",
      "Are they the same?\n",
      "  K match: True\n",
      "  V match: True\n"
     ]
    }
   ],
   "source": [
    "# Let's verify the cache was extracted correctly\n",
    "print(\"Original output cache info:\")\n",
    "print(\"  Number of layers:\", len(outputs.past_key_values))\n",
    "print(\"  Layer 0 K shape:\", outputs.past_key_values[0][0].shape)\n",
    "print(\"  Layer 0 V shape:\", outputs.past_key_values[0][1].shape)\n",
    "\n",
    "print(\"\\nExtracted cache info:\")\n",
    "print(\"  Number of layers:\", len(past))\n",
    "print(\"  Layer 0 K shape:\", past[0][0].shape)\n",
    "print(\"  Layer 0 V shape:\", past[0][1].shape)\n",
    "\n",
    "# Verify the extraction is correct by comparing\n",
    "print(\"\\nAre they the same?\")\n",
    "print(\"  K match:\", torch.allclose(outputs.past_key_values[0][0][:, :, :first_prompt_len, :], past[0][0]))\n",
    "print(\"  V match:\", torch.allclose(outputs.past_key_values[0][1][:, :, :first_prompt_len, :], past[0][1]))\n",
    "\n",
    "# Now let's try a simpler approach: just continue from the original cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation:\n",
      "  Cache length: 5\n",
      "First token: 1 -> '<eos>'\n",
      "\n",
      "Final generated: \n"
     ]
    }
   ],
   "source": [
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "def decode_with_kv_final(tokenizer, model, device, original_cache, prefix_len, max_new=30, temperature=1.0, top_k=50):\n",
    "    generated = []\n",
    "    \n",
    "    # Extract the FULL prefix cache (all 6 tokens)\n",
    "    cache = DynamicCache()\n",
    "    for layer_idx, (k, v) in enumerate(original_cache):\n",
    "        k_sliced = k[:, :, :prefix_len, :]\n",
    "        v_sliced = v[:, :, :prefix_len, :]\n",
    "        cache.update(k_sliced, v_sliced, layer_idx)\n",
    "    \n",
    "    print(f\"Starting generation:\")\n",
    "    print(f\"  Cache length: {cache.get_seq_length()}\")\n",
    "    \n",
    "    # To generate the NEXT token after the cached prefix, we need to:\n",
    "    # Option 1: Pass an empty/dummy token (doesn't work well)\n",
    "    # Option 2: Do one forward pass with the last cached token to get logits\n",
    "    \n",
    "    # Get logits for next token by looking at what was computed in original forward pass\n",
    "    # The logits at position [prefix_len-1] tell us what comes after the prefix\n",
    "    with torch.no_grad():\n",
    "        # Re-run just the last token with the cache of previous tokens\n",
    "        input_ids = tokens[\"input_ids\"][:, prefix_len-1:prefix_len].to(device)\n",
    "        \n",
    "        # Use cache of first prefix_len-1 tokens\n",
    "        cache_partial = DynamicCache()\n",
    "        for layer_idx, (k, v) in enumerate(original_cache):\n",
    "            k_sliced = k[:, :, :prefix_len-1, :]\n",
    "            v_sliced = v[:, :, :prefix_len-1, :]\n",
    "            cache_partial.update(k_sliced, v_sliced, layer_idx)\n",
    "        \n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=cache_partial,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Sample first token with temperature\n",
    "    logits = out.logits[:, -1, :] / temperature\n",
    "    \n",
    "    # Top-k sampling\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    generated.append(next_token.item())\n",
    "    print(f\"First token: {next_token.item()} -> '{tokenizer.decode(next_token[0])}'\")\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Continue generation with updated cache\n",
    "    cache = out.past_key_values\n",
    "    input_ids = next_token\n",
    "    \n",
    "    for step in range(1, max_new):\n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        # Sample with temperature\n",
    "        logits = out.logits[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if step < 5:\n",
    "            print(f\"Step {step}: token = {next_token.item()} -> '{tokenizer.decode(next_token[0])}'\")\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        input_ids = next_token\n",
    "        cache = out.past_key_values\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "result = decode_with_kv_final(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=\"cpu\",\n",
    "    original_cache=outputs.past_key_values,\n",
    "    prefix_len=first_prompt_len,\n",
    "    max_new=30,\n",
    "    temperature=0.8,\n",
    "    top_k=50\n",
    ")\n",
    "print(\"\\nFinal generated:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Process each prompt SEPARATELY (no packing)\n",
      "============================================================\n",
      "\n",
      "Processing prompt 1: 'What is machine learning?'\n",
      "  Token length: 5\n",
      "  Tokens: [3689, 563, 5464, 4735, 236881]\n",
      "  Cache shape: K=torch.Size([1, 1, 5, 256]), V=torch.Size([1, 1, 5, 256])\n",
      "\n",
      "Processing prompt 2: 'Tell me a joke about cats.'\n",
      "  Token length: 7\n",
      "  Tokens: [54593, 786, 496, 31481, 1003, 22797, 236761]\n",
      "  Cache shape: K=torch.Size([1, 1, 7, 256]), V=torch.Size([1, 1, 7, 256])\n",
      "\n",
      "Processing prompt 3: 'Who are you?'\n",
      "  Token length: 4\n",
      "  Tokens: [15938, 659, 611, 236881]\n",
      "  Cache shape: K=torch.Size([1, 1, 4, 256]), V=torch.Size([1, 1, 4, 256])\n",
      "\n",
      "============================================================\n",
      "STEP 2: Generate from each cached prompt\n",
      "============================================================\n",
      "\n",
      "  Prompt: 'What is machine learning?'\n",
      "  Cache length: 4\n",
      "  Last token: '?'\n",
      "  First generated: '?'\n",
      "  Generated: ???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "\n",
      "\n",
      "  Prompt: 'Tell me a joke about cats.'\n",
      "  Cache length: 6\n",
      "  Last token: '.'\n",
      "  First generated: '<eos>'\n",
      "  Generated: \n",
      "\n",
      "\n",
      "  Prompt: 'Who are you?'\n",
      "  Cache length: 3\n",
      "  Last token: '?'\n",
      "  First generated: ' your'\n",
      "  Generated:  your? your? your? your? your? your? your? your? your? your? your? your? your? your? your?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me a joke about cats.\",\n",
    "    \"Who are you?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Process each prompt SEPARATELY (no packing)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process each prompt individually to get clean KV caches\n",
    "all_caches = []\n",
    "all_prompt_lens = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nProcessing prompt {i+1}: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize this prompt alone\n",
    "    tokens_single = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    prompt_len = tokens_single[\"input_ids\"].shape[1]\n",
    "    all_prompt_lens.append(prompt_len)\n",
    "    \n",
    "    print(f\"  Token length: {prompt_len}\")\n",
    "    print(f\"  Tokens: {tokens_single['input_ids'][0].tolist()}\")\n",
    "    \n",
    "    # Forward pass with NORMAL attention (no blocking)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokens_single[\"input_ids\"],\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Extract and store the KV cache for this prompt\n",
    "    cache_for_prompt = []\n",
    "    for layer_kv in outputs.past_key_values:\n",
    "        k, v = layer_kv\n",
    "        cache_for_prompt.append((k, v))\n",
    "    \n",
    "    all_caches.append(cache_for_prompt)\n",
    "    \n",
    "    print(f\"  Cache shape: K={cache_for_prompt[0][0].shape}, V={cache_for_prompt[0][1].shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Generate from each cached prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def decode_with_clean_cache(prompt_text, cache_list, prompt_len, tokenizer, model, device, max_new=30, temperature=0.8, top_k=50):\n",
    "    \"\"\"Generate using a clean KV cache from individual prompt processing\"\"\"\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    # Convert to DynamicCache\n",
    "    cache_partial = DynamicCache()\n",
    "    for layer_idx, (k, v) in enumerate(cache_list):\n",
    "        # Use cache of first prompt_len-1 tokens\n",
    "        k_sliced = k[:, :, :prompt_len-1, :]\n",
    "        v_sliced = v[:, :, :prompt_len-1, :]\n",
    "        cache_partial.update(k_sliced, v_sliced, layer_idx)\n",
    "    \n",
    "    # Get the last token to process with the cache\n",
    "    tokens_single = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    input_ids = tokens_single[\"input_ids\"][:, prompt_len-1:prompt_len]\n",
    "    \n",
    "    print(f\"\\n  Prompt: '{prompt_text}'\")\n",
    "    print(f\"  Cache length: {cache_partial.get_seq_length()}\")\n",
    "    print(f\"  Last token: '{tokenizer.decode(input_ids[0])}'\")\n",
    "    \n",
    "    # First token generation\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=cache_partial,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Sample first token\n",
    "    logits = out.logits[:, -1, :] / temperature\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated.append(next_token.item())\n",
    "    \n",
    "    print(f\"  First generated: '{tokenizer.decode(next_token[0])}'\")\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Continue generation\n",
    "    cache = out.past_key_values\n",
    "    input_ids = next_token\n",
    "    \n",
    "    for step in range(1, max_new):\n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        logits = out.logits[:, -1, :] / temperature\n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        input_ids = next_token\n",
    "        cache = out.past_key_values\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Generate from each prompt's cache\n",
    "for i, (prompt, cache, prompt_len) in enumerate(zip(prompts, all_caches, all_prompt_lens)):\n",
    "    result = decode_with_clean_cache(\n",
    "        prompt_text=prompt,\n",
    "        cache_list=cache,\n",
    "        prompt_len=prompt_len,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        max_new=30,\n",
    "        temperature=0.8,\n",
    "        top_k=50\n",
    "    )\n",
    "    print(f\"  Generated: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEBUG: Compare normal generation vs KV cache generation\n",
      "============================================================\n",
      "\n",
      "Prompt: 'What is machine learning?'\n",
      "Token length: 5\n",
      "Tokens: [3689, 563, 5464, 4735, 236881]\n",
      "\n",
      "============================================================\n",
      "Test 1: NORMAL generation (no cache manipulation)\n",
      "============================================================\n",
      "Generated: ????????????????????\n",
      "\n",
      "============================================================\n",
      "Test 2: Forward pass and examine outputs\n",
      "============================================================\n",
      "Next token from logits: 236881 -> '?'\n",
      "\n",
      "============================================================\n",
      "Test 3: Manual generation with KV cache\n",
      "============================================================\n",
      "Cache length: 5\n",
      "Step 0: 236881 -> '?'\n",
      "Step 1: 236881 -> '?'\n",
      "Step 2: 236881 -> '?'\n",
      "Step 3: 236881 -> '?'\n",
      "Step 4: 236881 -> '?'\n",
      "\n",
      "Manual generation: ????????????????????\n",
      "\n",
      "============================================================\n",
      "Test 4: Your approach (partial cache + last token)\n",
      "============================================================\n",
      "Partial cache length: 4\n",
      "Last token: 236881 -> '?'\n",
      "Next token: 236881 -> '?'\n",
      "Step 1: 236881 -> '?'\n",
      "Step 2: 236881 -> '?'\n",
      "Step 3: 236881 -> '?'\n",
      "Step 4: 236881 -> '?'\n",
      "\n",
      "Partial cache generation: ????????????????????\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Normal generation:       ????????????????????\n",
      "Manual with full cache:  ????????????????????\n",
      "Your approach (partial): ????????????????????\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEBUG: Compare normal generation vs KV cache generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt = \"What is machine learning?\"\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "prompt_len = tokens[\"input_ids\"].shape[1]\n",
    "print(f\"Token length: {prompt_len}\")\n",
    "print(f\"Tokens: {tokens['input_ids'][0].tolist()}\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 1: Normal generation (baseline)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 1: NORMAL generation (no cache manipulation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_normal = model.generate(\n",
    "        tokens[\"input_ids\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,  # Greedy for reproducibility\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "generated_normal = tokenizer.decode(output_normal[0][prompt_len:], skip_special_tokens=True)\n",
    "print(f\"Generated: {generated_normal}\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 2: Get KV cache and check logits\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 2: Forward pass and examine outputs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=tokens[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Check what the next token SHOULD be according to the logits\n",
    "next_token_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "print(f\"Next token from logits: {next_token_id} -> '{tokenizer.decode([next_token_id])}'\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 3: Try to reproduce using manual KV cache\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 3: Manual generation with KV cache\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract cache\n",
    "cache = DynamicCache()\n",
    "for layer_idx, (k, v) in enumerate(outputs.past_key_values):\n",
    "    cache.update(k, v, layer_idx)\n",
    "\n",
    "print(f\"Cache length: {cache.get_seq_length()}\")\n",
    "\n",
    "# Now try to generate next token using the cache\n",
    "# The cache already contains ALL tokens, so we just need to sample from logits\n",
    "generated_ids = []\n",
    "\n",
    "# First token: use the logits we already have\n",
    "next_token = torch.argmax(outputs.logits[0, -1, :], keepdim=True).unsqueeze(0)\n",
    "generated_ids.append(next_token.item())\n",
    "print(f\"Step 0: {next_token.item()} -> '{tokenizer.decode(next_token[0])}'\")\n",
    "\n",
    "# Continue generation\n",
    "input_ids = next_token\n",
    "for step in range(1, 20):\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=cache,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    next_token = torch.argmax(out.logits[0, -1, :], keepdim=True).unsqueeze(0)\n",
    "    generated_ids.append(next_token.item())\n",
    "    \n",
    "    if step < 5:\n",
    "        print(f\"Step {step}: {next_token.item()} -> '{tokenizer.decode(next_token[0])}'\")\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        break\n",
    "    \n",
    "    input_ids = next_token\n",
    "    cache = out.past_key_values\n",
    "\n",
    "generated_manual = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"\\nManual generation: {generated_manual}\")\n",
    "\n",
    "# ============================================================\n",
    "# Test 4: What you were trying to do (partial cache)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 4: Your approach (partial cache + last token)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create partial cache (first n-1 tokens)\n",
    "cache_partial = DynamicCache()\n",
    "for layer_idx, (k, v) in enumerate(outputs.past_key_values):\n",
    "    k_sliced = k[:, :, :prompt_len-1, :]\n",
    "    v_sliced = v[:, :, :prompt_len-1, :]\n",
    "    cache_partial.update(k_sliced, v_sliced, layer_idx)\n",
    "\n",
    "print(f\"Partial cache length: {cache_partial.get_seq_length()}\")\n",
    "\n",
    "# Process last token with partial cache\n",
    "last_token = tokens[\"input_ids\"][:, -1:].to(device)\n",
    "print(f\"Last token: {last_token.item()} -> '{tokenizer.decode(last_token[0])}'\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(\n",
    "        input_ids=last_token,\n",
    "        past_key_values=cache_partial,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "next_token_id = torch.argmax(out.logits[0, -1, :]).item()\n",
    "print(f\"Next token: {next_token_id} -> '{tokenizer.decode([next_token_id])}'\")\n",
    "\n",
    "# Continue generation\n",
    "generated_ids_partial = [next_token_id]\n",
    "input_ids = torch.tensor([[next_token_id]], device=device)\n",
    "cache_new = out.past_key_values\n",
    "\n",
    "for step in range(1, 20):\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=cache_new,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    next_token = torch.argmax(out.logits[0, -1, :]).item()\n",
    "    generated_ids_partial.append(next_token)\n",
    "    \n",
    "    if step < 5:\n",
    "        print(f\"Step {step}: {next_token} -> '{tokenizer.decode([next_token])}'\")\n",
    "    \n",
    "    if next_token == tokenizer.eos_token_id:\n",
    "        break\n",
    "    \n",
    "    input_ids = torch.tensor([[next_token]], device=device)\n",
    "    cache_new = out.past_key_values\n",
    "\n",
    "generated_partial = tokenizer.decode(generated_ids_partial, skip_special_tokens=True)\n",
    "print(f\"\\nPartial cache generation: {generated_partial}\")\n",
    "\n",
    "# ============================================================\n",
    "# Summary\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Normal generation:       {generated_normal}\")\n",
    "print(f\"Manual with full cache:  {generated_manual}\")\n",
    "print(f\"Your approach (partial): {generated_partial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP TOKEN: <eos>\n",
      "BOS TOKEN ID: 2\n",
      "\n",
      "============================================================\n",
      "Processing prompts with proper tokenization\n",
      "============================================================\n",
      "\n",
      "Prompt 1: 'What is machine learning?'\n",
      "  Tokens: [2, 3689, 563, 5464, 4735, 236881]\n",
      "  Length: 6\n",
      "  Next token prediction: '\n",
      "\n",
      "'\n",
      "\n",
      "Prompt 2: 'Tell me a joke about cats.'\n",
      "  Tokens: [2, 54593, 786, 496, 31481, 1003, 22797, 236761]\n",
      "  Length: 8\n",
      "  Next token prediction: '\n",
      "\n",
      "'\n",
      "\n",
      "Prompt 3: 'Who are you?'\n",
      "  Tokens: [2, 15938, 659, 611, 236881]\n",
      "  Length: 5\n",
      "  Next token prediction: '\n",
      "\n",
      "'\n",
      "\n",
      "============================================================\n",
      "Generating from cached prompts\n",
      "============================================================\n",
      "\n",
      "Prompt 1: 'What is machine learning?'\n",
      "  Cache length: 6\n",
      "  First token: '\n",
      "\n",
      "'\n",
      "  Step 1: 'Machine'\n",
      "  Step 2: ' learning'\n",
      "  Generated: \n",
      "\n",
      "Machine learning is a technique that has helped us develop a lot of important things in technology. But, the term, machine learning, is not actually\n",
      "\n",
      "\n",
      "Prompt 2: 'Tell me a joke about cats.'\n",
      "  Cache length: 8\n",
      "  First token: '\n",
      "\n",
      "'\n",
      "  Step 1: 'If'\n",
      "  Step 2: ' I'\n",
      "  Generated: \n",
      "\n",
      "If I did, the joke would be:\n",
      "\n",
      "“cats don't know what they're talking about. They just know their owners don\n",
      "\n",
      "\n",
      "Prompt 3: 'Who are you?'\n",
      "  Cache length: 5\n",
      "  First token: '\n",
      "\n",
      "'\n",
      "  Step 1: 'What'\n",
      "  Step 2: ' do'\n",
      "  Generated: \n",
      "\n",
      "What do you do?\n",
      "\n",
      "What do you think?\n",
      "\n",
      "What makes you different than\n",
      "\n",
      "others?\n",
      "\n",
      "What do you like?\n",
      "\n",
      "What does\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "# ✅ Format prompts properly for Gemma\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me a joke about cats.\",\n",
    "    \"Who are you?\"\n",
    "]\n",
    "\n",
    "sep = tokenizer.eos_token\n",
    "print(\"SEP TOKEN:\", sep)\n",
    "print(\"BOS TOKEN ID:\", tokenizer.bos_token_id)\n",
    "\n",
    "# ============================================================\n",
    "# CORRECT APPROACH: Add BOS token and process individually\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Processing prompts with proper tokenization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_caches = []\n",
    "all_prompt_lens = []\n",
    "all_tokens = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nPrompt {i+1}: '{prompt}'\")\n",
    "    \n",
    "    # ✅ Add special tokens (BOS)\n",
    "    tokens_single = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    prompt_len = tokens_single[\"input_ids\"].shape[1]\n",
    "    all_prompt_lens.append(prompt_len)\n",
    "    all_tokens.append(tokens_single[\"input_ids\"])\n",
    "    \n",
    "    print(f\"  Tokens: {tokens_single['input_ids'][0].tolist()}\")\n",
    "    print(f\"  Length: {prompt_len}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokens_single[\"input_ids\"],\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Check what it wants to generate\n",
    "    next_token_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "    print(f\"  Next token prediction: '{tokenizer.decode([next_token_id])}'\")\n",
    "    \n",
    "    # Store cache\n",
    "    cache_for_prompt = []\n",
    "    for layer_kv in outputs.past_key_values:\n",
    "        k, v = layer_kv\n",
    "        cache_for_prompt.append((k, v))\n",
    "    \n",
    "    all_caches.append(cache_for_prompt)\n",
    "\n",
    "# ============================================================\n",
    "# Generate from each cached prompt\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating from cached prompts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_from_cache(prompt_tokens, cache_list, prompt_len, tokenizer, model, device, max_new=30):\n",
    "    \"\"\"Generate using KV cache\"\"\"\n",
    "    generated = []\n",
    "    \n",
    "    # Convert to DynamicCache (use full cache this time)\n",
    "    cache = DynamicCache()\n",
    "    for layer_idx, (k, v) in enumerate(cache_list):\n",
    "        cache.update(k, v, layer_idx)\n",
    "    \n",
    "    print(f\"  Cache length: {cache.get_seq_length()}\")\n",
    "    \n",
    "    # Use the logits from the initial forward pass to get first token\n",
    "    # We need to recompute to get the logits\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=prompt_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Get first token\n",
    "    next_token = torch.argmax(out.logits[0, -1, :], keepdim=True).unsqueeze(0)\n",
    "    generated.append(next_token.item())\n",
    "    \n",
    "    print(f\"  First token: '{tokenizer.decode(next_token[0])}'\")\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Continue with cache\n",
    "    input_ids = next_token\n",
    "    cache = out.past_key_values\n",
    "    \n",
    "    for step in range(1, max_new):\n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        # Use sampling for better results\n",
    "        logits = out.logits[0, -1, :] / 0.8\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Top-k sampling\n",
    "        top_k = 50\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if step < 3:\n",
    "            print(f\"  Step {step}: '{tokenizer.decode(next_token[0])}'\")\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        input_ids = next_token.unsqueeze(0)\n",
    "        cache = out.past_key_values\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Generate from each\n",
    "for i, (prompt, cache, prompt_len, tokens) in enumerate(zip(prompts, all_caches, all_prompt_lens, all_tokens)):\n",
    "    print(f\"\\nPrompt {i+1}: '{prompt}'\")\n",
    "    result = generate_from_cache(\n",
    "        prompt_tokens=tokens,\n",
    "        cache_list=cache,\n",
    "        prompt_len=prompt_len,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        max_new=30\n",
    "    )\n",
    "    print(f\"  Generated: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT PACKING WITH KV CACHE EXTRACTION\n",
      "============================================================\n",
      "\n",
      "Step 1: Tokenize prompts individually (to get lengths)\n",
      "  Prompt 1: 'What is machine learning?' -> 6 tokens\n",
      "  Prompt 2: 'Can you tell me a joke about cats.' -> 10 tokens\n",
      "  Prompt 3: 'Do you know anything about India?' -> 8 tokens\n",
      "\n",
      "Step 2: Pack prompts into single sequence\n",
      "  Packed sequence length: 26\n",
      "  Prompt ranges: [(0, 6), (7, 17), (18, 26)]\n",
      "\n",
      "Step 3: Build block-diagonal attention mask\n",
      "  Attention mask shape: torch.Size([1, 4, 26, 26])\n",
      "\n",
      "Step 4: Forward pass with prompt packing\n",
      "  ✓ Forward pass complete\n",
      "\n",
      "Step 5: Extract KV caches for each prompt\n",
      "  Prompt 1: Extracted cache with 6 tokens\n",
      "  Prompt 2: Extracted cache with 10 tokens\n",
      "  Prompt 3: Extracted cache with 8 tokens\n",
      "\n",
      "============================================================\n",
      "Step 6: Generate from extracted caches\n",
      "============================================================\n",
      "\n",
      "  Prompt: 'What is machine learning?'\n",
      "  Prompt length: 6 tokens\n",
      "  Using cache length: 6\n",
      "  Processing last token: '?'\n",
      "  First generated token: '\n",
      "\n",
      "'\n",
      "  Step 1: 'A'\n",
      "  Step 2: ' method'\n",
      "  Generated: \n",
      "\n",
      "A method is a series of steps to improve an activity. It can be applied to learning a task, or it can be applied to performance in a specific field. In short, it is a method to train a machine that can perform an activity\n",
      "\n",
      "\n",
      "  Prompt: 'Can you tell me a joke about cats.'\n",
      "  Prompt length: 10 tokens\n",
      "  Using cache length: 10\n",
      "  Processing last token: '.'\n",
      "  First generated token: '\n",
      "'\n",
      "  Step 1: '('\n",
      "  Step 2: 'cat'\n",
      "  Generated: \n",
      "(cat?\n",
      "Can you tell me when I was 16\n",
      "(What was I doing?)\n",
      "(Where is your cat? Why do you do it?\n",
      "(Is it a cat or a tree?\n",
      "(Is it a\n",
      "\n",
      "\n",
      "  Prompt: 'Do you know anything about India?'\n",
      "  Prompt length: 8 tokens\n",
      "  Using cache length: 8\n",
      "  Processing last token: '?'\n",
      "  First generated token: ' And'\n",
      "  Step 1: ' if'\n",
      "  Step 2: ' you'\n",
      "  Generated:  And if you?s have a topic you need to know about your?s history? Does a topic inspire you? You need to know India and it’s people!\n",
      "\n",
      "India is a country of millions of Indians who will be visiting India in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you tell me a joke about cats.\",\n",
    "    \"Do you know anything about India?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT PACKING WITH KV CACHE EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: Tokenize each prompt WITH BOS token\n",
    "# ============================================================\n",
    "print(\"\\nStep 1: Tokenize prompts individually (to get lengths)\")\n",
    "\n",
    "prompt_tokens = []\n",
    "prompt_lengths = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    prompt_tokens.append(tokens[\"input_ids\"][0])\n",
    "    prompt_lengths.append(len(tokens[\"input_ids\"][0]))\n",
    "    print(f\"  Prompt {i+1}: '{prompt}' -> {prompt_lengths[i]} tokens\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: Pack prompts (concatenate with proper separation)\n",
    "# ============================================================\n",
    "print(\"\\nStep 2: Pack prompts into single sequence\")\n",
    "\n",
    "# Concatenate all prompts (they already have BOS, no need for separator between)\n",
    "# OR use EOS as separator if you want explicit boundaries\n",
    "sep_token_id = tokenizer.eos_token_id\n",
    "\n",
    "packed_tokens = []\n",
    "prompt_ranges = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, tokens in enumerate(prompt_tokens):\n",
    "    start = current_pos\n",
    "    end = current_pos + len(tokens)\n",
    "    prompt_ranges.append((start, end))\n",
    "    \n",
    "    packed_tokens.extend(tokens.tolist())\n",
    "    \n",
    "    # Add separator after each prompt (except last)\n",
    "    if i < len(prompt_tokens) - 1:\n",
    "        packed_tokens.append(sep_token_id)\n",
    "        current_pos = end + 1  # +1 for separator\n",
    "    else:\n",
    "        current_pos = end\n",
    "\n",
    "# Convert to tensor\n",
    "packed_input_ids = torch.tensor([packed_tokens], device=device)\n",
    "seq_len = packed_input_ids.shape[1]\n",
    "\n",
    "print(f\"  Packed sequence length: {seq_len}\")\n",
    "print(f\"  Prompt ranges: {prompt_ranges}\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Build block-diagonal attention mask\n",
    "# ============================================================\n",
    "print(\"\\nStep 3: Build block-diagonal attention mask\")\n",
    "\n",
    "attn_mask = torch.zeros((seq_len, seq_len), dtype=torch.float, device=device)\n",
    "\n",
    "for start, end in prompt_ranges:\n",
    "    attn_mask[start:end, start:end] = 1.0\n",
    "\n",
    "# Expand for multi-head attention\n",
    "num_heads = model.config.num_attention_heads\n",
    "attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
    "attn_mask = attn_mask.repeat(1, num_heads, 1, 1)  # [1, heads, seq, seq]\n",
    "\n",
    "print(f\"  Attention mask shape: {attn_mask.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 4: Single forward pass with packed prompts\n",
    "# ============================================================\n",
    "print(\"\\nStep 4: Forward pass with prompt packing\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=packed_input_ids,\n",
    "        attention_mask=attn_mask,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(\"  ✓ Forward pass complete\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Extract individual KV caches for each prompt\n",
    "# ============================================================\n",
    "print(\"\\nStep 5: Extract KV caches for each prompt\")\n",
    "\n",
    "all_caches = []\n",
    "\n",
    "for i, (start, end) in enumerate(prompt_ranges):\n",
    "    cache_for_prompt = []\n",
    "    \n",
    "    for layer_kv in outputs.past_key_values:\n",
    "        k, v = layer_kv  # Shape: [batch, heads, seq, head_dim]\n",
    "        \n",
    "        # Extract only this prompt's tokens\n",
    "        k_sliced = k[:, :, start:end, :]\n",
    "        v_sliced = v[:, :, start:end, :]\n",
    "        \n",
    "        cache_for_prompt.append((k_sliced, v_sliced))\n",
    "    \n",
    "    all_caches.append(cache_for_prompt)\n",
    "    print(f\"  Prompt {i+1}: Extracted cache with {end-start} tokens\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 6: Generate from each extracted cache\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Step 6: Generate from extracted caches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_from_extracted_cache(prompt_text, cache_list, tokenizer, model, device, max_new=30):\n",
    "    \"\"\"Generate using extracted KV cache from packed batch\"\"\"\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    # Tokenize prompt to know the token count\n",
    "    tokens = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    prompt_len = tokens[\"input_ids\"].shape[1]\n",
    "    \n",
    "    print(f\"\\n  Prompt: '{prompt_text}'\")\n",
    "    print(f\"  Prompt length: {prompt_len} tokens\")\n",
    "    \n",
    "    # ✅ CORRECT: Use cache of first (n-1) tokens, process last token\n",
    "    cache_partial = DynamicCache()\n",
    "    for layer_idx, (k, v) in enumerate(cache_list):\n",
    "        # Use all but the last token\n",
    "        k_partial = k[:, :, :, :]\n",
    "        v_partial = v[:, :, :, :]\n",
    "        cache_partial.update(k_partial, v_partial, layer_idx)\n",
    "    \n",
    "    print(f\"  Using cache length: {cache_partial.get_seq_length()}\")\n",
    "    \n",
    "    # Process only the LAST token with the cached context\n",
    "    last_token = tokens[\"input_ids\"][:, -1:]\n",
    "    print(f\"  Processing last token: '{tokenizer.decode(last_token[0])}'\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=last_token,\n",
    "            past_key_values=cache_partial,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Sample first NEW token\n",
    "    logits = out.logits[0, -1, :] / 0.8\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Top-k sampling\n",
    "    top_k = 50\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated.append(next_token.item())\n",
    "    \n",
    "    print(f\"  First generated token: '{tokenizer.decode(next_token[0])}'\")\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Continue generation with the updated cache\n",
    "    cache = out.past_key_values\n",
    "    input_ids = next_token.unsqueeze(0)\n",
    "    \n",
    "    for step in range(1, max_new):\n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        logits = out.logits[0, -1, :] / 0.8\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if step < 3:\n",
    "            print(f\"  Step {step}: '{tokenizer.decode(next_token[0])}'\")\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        input_ids = next_token.unsqueeze(0)\n",
    "        cache = out.past_key_values\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Generate from each extracted cache\n",
    "for i, (prompt, cache) in enumerate(zip(prompts, all_caches)):\n",
    "    result = generate_from_extracted_cache(\n",
    "        prompt_text=prompt,\n",
    "        cache_list=cache,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        max_new=50\n",
    "    )\n",
    "    print(f\"  Generated: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT PACKING WITH KV CACHE EXTRACTION\n",
      "============================================================\n",
      "\n",
      "Step 1: Tokenize prompts individually (to get lengths)\n",
      "  Prompt 1: 'What is machine learning?' -> 6 tokens\n",
      "  Prompt 2: 'Can you tell me a joke about cats.' -> 10 tokens\n",
      "  Prompt 3: 'Do you know anything about India?' -> 8 tokens\n",
      "\n",
      "Step 2: Pack prompts into single sequence\n",
      "  Packed sequence length: 26\n",
      "  Prompt ranges: [(0, 6), (7, 17), (18, 26)]\n",
      "\n",
      "Step 3: Build block-diagonal attention mask\n",
      "  Attention mask shape: torch.Size([1, 4, 26, 26])\n",
      "\n",
      "Step 4: Forward pass with prompt packing\n",
      "  ✓ Forward pass complete\n",
      "\n",
      "Step 5: Extract KV caches for each prompt\n",
      "  Prompt 1: Extracted cache with 6 tokens\n",
      "  Prompt 2: Extracted cache with 10 tokens\n",
      "  Prompt 3: Extracted cache with 8 tokens\n",
      "\n",
      "============================================================\n",
      "Step 6: Generate from extracted caches\n",
      "============================================================\n",
      "\n",
      "  Prompt: 'What is machine learning?'\n",
      "  Prompt length: 6 tokens\n",
      "  Using cache length: 6\n",
      "  Processing last token: '?'\n",
      "  Position ID for last token: 5\n",
      "  First generated token: '\n",
      "\n",
      "'\n",
      "  Step 1 (pos 6): 'This'\n",
      "  Step 2 (pos 7): ' is'\n",
      "  Generated: \n",
      "\n",
      "This is a computer which has a lot of data to process and perform a lot of tasks. The most basic of these tasks is to predict future events.\n",
      "For the machine learning task, an important factor is the distribution of the data. One\n",
      "\n",
      "\n",
      "  Prompt: 'Can you tell me a joke about cats.'\n",
      "  Prompt length: 10 tokens\n",
      "  Using cache length: 10\n",
      "  Processing last token: '.'\n",
      "  Position ID for last token: 9\n",
      "  First generated token: '\n",
      "'\n",
      "  Step 1 (pos 10): 'I'\n",
      "  Step 2 (pos 11): ' want'\n",
      "  Generated: \n",
      "I want to be a cat of course.\n",
      "I'm not sure what you are talking about.\n",
      "Cats have a distinctively different scent than dogs. For example, cats can make a very\n",
      "distinctive smell like smoke or a very\n",
      "\n",
      "\n",
      "  Prompt: 'Do you know anything about India?'\n",
      "  Prompt length: 8 tokens\n",
      "  Using cache length: 8\n",
      "  Processing last token: '?'\n",
      "  Position ID for last token: 7\n",
      "  First generated token: '\n",
      "\n",
      "'\n",
      "  Step 1 (pos 8): 'About'\n",
      "  Step 2 (pos 9): ' India'\n",
      "  Generated: \n",
      "\n",
      "About India?\n",
      "\n",
      "I?\n",
      "\n",
      "It?\n",
      "\n",
      "I?\n",
      "\n",
      "About India?\n",
      "\n",
      "Yes, I do, I am fluent in it and I have lived in India for many years now.\n",
      "\n",
      "I have read a lot about India. I have\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you tell me a joke about cats.\",\n",
    "    \"Do you know anything about India?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT PACKING WITH KV CACHE EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: Tokenize each prompt WITH BOS token\n",
    "# ============================================================\n",
    "print(\"\\nStep 1: Tokenize prompts individually (to get lengths)\")\n",
    "\n",
    "prompt_tokens = []\n",
    "prompt_lengths = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    prompt_tokens.append(tokens[\"input_ids\"][0])\n",
    "    prompt_lengths.append(len(tokens[\"input_ids\"][0]))\n",
    "    print(f\"  Prompt {i+1}: '{prompt}' -> {prompt_lengths[i]} tokens\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: Pack prompts (concatenate with proper separation)\n",
    "# ============================================================\n",
    "print(\"\\nStep 2: Pack prompts into single sequence\")\n",
    "\n",
    "# Concatenate all prompts (they already have BOS, no need for separator between)\n",
    "# OR use EOS as separator if you want explicit boundaries\n",
    "sep_token_id = tokenizer.eos_token_id\n",
    "\n",
    "packed_tokens = []\n",
    "prompt_ranges = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, tokens in enumerate(prompt_tokens):\n",
    "    start = current_pos\n",
    "    end = current_pos + len(tokens)\n",
    "    prompt_ranges.append((start, end))\n",
    "    \n",
    "    packed_tokens.extend(tokens.tolist())\n",
    "    \n",
    "    # Add separator after each prompt (except last)\n",
    "    if i < len(prompt_tokens) - 1:\n",
    "        packed_tokens.append(sep_token_id)\n",
    "        current_pos = end + 1  # +1 for separator\n",
    "    else:\n",
    "        current_pos = end\n",
    "\n",
    "# Convert to tensor\n",
    "packed_input_ids = torch.tensor([packed_tokens], device=device)\n",
    "seq_len = packed_input_ids.shape[1]\n",
    "\n",
    "print(f\"  Packed sequence length: {seq_len}\")\n",
    "print(f\"  Prompt ranges: {prompt_ranges}\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Build block-diagonal attention mask\n",
    "# ============================================================\n",
    "print(\"\\nStep 3: Build block-diagonal attention mask\")\n",
    "\n",
    "attn_mask = torch.zeros((seq_len, seq_len), dtype=torch.float, device=device)\n",
    "\n",
    "for start, end in prompt_ranges:\n",
    "    attn_mask[start:end, start:end] = 1.0\n",
    "\n",
    "# Expand for multi-head attention\n",
    "num_heads = model.config.num_attention_heads\n",
    "attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
    "attn_mask = attn_mask.repeat(1, num_heads, 1, 1)  # [1, heads, seq, seq]\n",
    "\n",
    "print(f\"  Attention mask shape: {attn_mask.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 4: Single forward pass with packed prompts\n",
    "# ============================================================\n",
    "print(\"\\nStep 4: Forward pass with prompt packing\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=packed_input_ids,\n",
    "        attention_mask=attn_mask,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "print(\"  ✓ Forward pass complete\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Extract individual KV caches for each prompt\n",
    "# ============================================================\n",
    "print(\"\\nStep 5: Extract KV caches for each prompt\")\n",
    "\n",
    "all_caches = []\n",
    "\n",
    "for i, (start, end) in enumerate(prompt_ranges):\n",
    "    cache_for_prompt = []\n",
    "    \n",
    "    for layer_kv in outputs.past_key_values:\n",
    "        k, v = layer_kv  # Shape: [batch, heads, seq, head_dim]\n",
    "        \n",
    "        # Extract only this prompt's tokens\n",
    "        k_sliced = k[:, :, start:end, :]\n",
    "        v_sliced = v[:, :, start:end, :]\n",
    "        \n",
    "        cache_for_prompt.append((k_sliced, v_sliced))\n",
    "    \n",
    "    all_caches.append(cache_for_prompt)\n",
    "    print(f\"  Prompt {i+1}: Extracted cache with {end-start} tokens\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 6: Generate from each extracted cache\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Step 6: Generate from extracted caches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_from_extracted_cache(prompt_text, cache_list, tokenizer, model, device, max_new=30):\n",
    "    \"\"\"Generate using extracted KV cache from packed batch\"\"\"\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    # Tokenize prompt to know the token count\n",
    "    tokens = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    prompt_len = tokens[\"input_ids\"].shape[1]\n",
    "    \n",
    "    print(f\"\\n  Prompt: '{prompt_text}'\")\n",
    "    print(f\"  Prompt length: {prompt_len} tokens\")\n",
    "    \n",
    "    # Convert to DynamicCache\n",
    "    cache_partial = DynamicCache()\n",
    "    for layer_idx, (k, v) in enumerate(cache_list):\n",
    "        cache_partial.update(k, v, layer_idx)\n",
    "    \n",
    "    cache_len = cache_partial.get_seq_length()\n",
    "    print(f\"  Using cache length: {cache_len}\")\n",
    "    \n",
    "    # Process only the LAST token with the cached context\n",
    "    last_token = tokens[\"input_ids\"][:, -1:]\n",
    "    print(f\"  Processing last token: '{tokenizer.decode(last_token[0])}'\")\n",
    "    \n",
    "    # ✅ EXPLICIT position_ids for the last token\n",
    "    # Since cache has cache_len tokens (positions 0 to cache_len-1),\n",
    "    # the last token should be at position cache_len-1\n",
    "    position_ids = torch.tensor([[cache_len - 1]], device=device)\n",
    "    print(f\"  Position ID for last token: {position_ids.item()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=last_token,\n",
    "            past_key_values=cache_partial,\n",
    "            position_ids=position_ids,  # ✅ Explicit position\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Sample first NEW token\n",
    "    logits = out.logits[0, -1, :] / 0.8\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Top-k sampling\n",
    "    top_k = 50\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated.append(next_token.item())\n",
    "    \n",
    "    print(f\"  First generated token: '{tokenizer.decode(next_token[0])}'\")\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Continue generation with the updated cache\n",
    "    cache = out.past_key_values\n",
    "    input_ids = next_token.unsqueeze(0)\n",
    "    current_position = cache_len  # Next position after cache\n",
    "    \n",
    "    for step in range(1, max_new):\n",
    "        # ✅ EXPLICIT position_ids for new tokens\n",
    "        position_ids = torch.tensor([[current_position]], device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=cache,\n",
    "                position_ids=position_ids,  # ✅ Explicit position\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        logits = out.logits[0, -1, :] / 0.8\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if step < 3:\n",
    "            print(f\"  Step {step} (pos {current_position}): '{tokenizer.decode(next_token[0])}'\")\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        input_ids = next_token.unsqueeze(0)\n",
    "        cache = out.past_key_values\n",
    "        current_position += 1  # ✅ Increment position\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Generate from each extracted cache\n",
    "for i, (prompt, cache) in enumerate(zip(prompts, all_caches)):\n",
    "    result = generate_from_extracted_cache(\n",
    "        prompt_text=prompt,\n",
    "        cache_list=cache,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        max_new=50\n",
    "    )\n",
    "    print(f\"  Generated: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT PACKING WITH KV CACHE EXTRACTION\n",
      "============================================================\n",
      "\n",
      "Step 1: Pack prompts into single sequence\n",
      "  Packed sequence length: 26\n",
      "  Prompt ranges: [(0, 6), (7, 17), (18, 26)]\n",
      "    Prompt 1: 'What is machine learning?' -> 6 tokens\n",
      "    Prompt 2: 'Can you tell me a joke about cats.' -> 10 tokens\n",
      "    Prompt 3: 'Do you know anything about India?' -> 8 tokens\n",
      "\n",
      "Step 2: Build block-diagonal attention mask\n",
      "  Attention mask shape: torch.Size([1, 4, 26, 26])\n",
      "\n",
      "Step 3: Forward pass with prompt packing\n",
      "  ✓ Forward pass complete\n",
      "\n",
      "Step 4: Extract KV caches for each prompt\n",
      "  Prompt 1: Extracted cache with 6 tokens\n",
      "  Prompt 2: Extracted cache with 10 tokens\n",
      "  Prompt 3: Extracted cache with 8 tokens\n",
      "\n",
      "============================================================\n",
      "Step 5: Generate from extracted caches\n",
      "============================================================\n",
      "\n",
      "Prompt 1: 'What is machine learning?'\n",
      "Generated: \n",
      "Machine learning is a technique that uses algorithms to extract or train information from data that is available in the real world. This technique is often used in various fields, including computer vision, natural language processing, and data mining. In machine learning, algorithms\n",
      "\n",
      "\n",
      "Prompt 2: 'Can you tell me a joke about cats.'\n",
      "Generated: \n",
      "me which?\n",
      "Why don't cats do like\n",
      "to eat food\n",
      "But cats I know\n",
      "Cats you know\n",
      "Cats I don't know\n",
      "Cats?\n",
      "What's the point\n",
      "of the cat?\n",
      "Cats\n",
      "The\n",
      "\n",
      "\n",
      "Prompt 3: 'Do you know anything about India?'\n",
      "Generated:  <strong>about 999919999999999999? India is in the country, and it is called India. India is a country in southern Asia. India is a country. India is\n",
      "\n",
      "============================================================\n",
      "✓ Prompt packing with KV cache extraction complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "def pack_prompts(prompts, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Pack multiple prompts into a single sequence with separators.\n",
    "    \n",
    "    Returns:\n",
    "        packed_input_ids: Tensor of packed token IDs\n",
    "        prompt_ranges: List of (start, end) tuples for each prompt\n",
    "    \"\"\"\n",
    "    prompt_tokens = []\n",
    "    prompt_lengths = []\n",
    "    \n",
    "    # Tokenize each prompt\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        prompt_tokens.append(tokens[\"input_ids\"][0])\n",
    "        prompt_lengths.append(len(tokens[\"input_ids\"][0]))\n",
    "    \n",
    "    # Pack prompts with EOS separator\n",
    "    sep_token_id = tokenizer.eos_token_id\n",
    "    packed_tokens = []\n",
    "    prompt_ranges = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for i, tokens in enumerate(prompt_tokens):\n",
    "        start = current_pos\n",
    "        end = current_pos + len(tokens)\n",
    "        prompt_ranges.append((start, end))\n",
    "        \n",
    "        packed_tokens.extend(tokens.tolist())\n",
    "        \n",
    "        # Add separator after each prompt (except last)\n",
    "        if i < len(prompt_tokens) - 1:\n",
    "            packed_tokens.append(sep_token_id)\n",
    "            current_pos = end + 1\n",
    "        else:\n",
    "            current_pos = end\n",
    "    \n",
    "    packed_input_ids = torch.tensor([packed_tokens], device=device)\n",
    "    \n",
    "    return packed_input_ids, prompt_ranges, prompt_lengths\n",
    "\n",
    "\n",
    "def create_block_diagonal_mask(seq_len, prompt_ranges, num_heads, device):\n",
    "    \"\"\"\n",
    "    Create block-diagonal attention mask to prevent cross-attention between prompts.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Total sequence length\n",
    "        prompt_ranges: List of (start, end) tuples for each prompt\n",
    "        num_heads: Number of attention heads\n",
    "        device: Device to create tensor on\n",
    "        \n",
    "    Returns:\n",
    "        attention_mask: Tensor of shape [1, num_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    attn_mask = torch.zeros((seq_len, seq_len), dtype=torch.float, device=device)\n",
    "    \n",
    "    # Fill diagonal blocks for each prompt\n",
    "    for start, end in prompt_ranges:\n",
    "        attn_mask[start:end, start:end] = 1.0\n",
    "    \n",
    "    # Expand for multi-head attention\n",
    "    attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
    "    attn_mask = attn_mask.repeat(1, num_heads, 1, 1)  # [1, heads, seq, seq]\n",
    "    \n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "def extract_prompt_caches(past_key_values, prompt_ranges):\n",
    "    \"\"\"\n",
    "    Extract individual KV caches for each prompt from packed forward pass.\n",
    "    \n",
    "    Args:\n",
    "        past_key_values: KV cache from model forward pass\n",
    "        prompt_ranges: List of (start, end) tuples for each prompt\n",
    "        \n",
    "    Returns:\n",
    "        all_caches: List of cache_list for each prompt\n",
    "    \"\"\"\n",
    "    all_caches = []\n",
    "    \n",
    "    for start, end in prompt_ranges:\n",
    "        cache_for_prompt = []\n",
    "        \n",
    "        for layer_kv in past_key_values:\n",
    "            k, v = layer_kv  # Shape: [batch, heads, seq, head_dim]\n",
    "            \n",
    "            # Extract only this prompt's tokens\n",
    "            k_sliced = k[:, :, start:end, :]\n",
    "            v_sliced = v[:, :, start:end, :]\n",
    "            \n",
    "            cache_for_prompt.append((k_sliced, v_sliced))\n",
    "        \n",
    "        all_caches.append(cache_for_prompt)\n",
    "    \n",
    "    return all_caches\n",
    "\n",
    "\n",
    "def generate_from_cache(\n",
    "    prompt_text,\n",
    "    cache_list,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    max_new=50,\n",
    "    temperature=0.8,\n",
    "    top_k=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using extracted KV cache from packed batch.\n",
    "    \n",
    "    Args:\n",
    "        prompt_text: Original prompt text\n",
    "        cache_list: List of (K, V) tuples for each layer\n",
    "        tokenizer: Tokenizer instance\n",
    "        model: Model instance\n",
    "        device: Device\n",
    "        max_new: Maximum new tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k sampling parameter\n",
    "        \n",
    "    Returns:\n",
    "        generated_text: Generated text string\n",
    "    \"\"\"\n",
    "    generated = []\n",
    "    \n",
    "    # Tokenize prompt to know token count\n",
    "    tokens = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    prompt_len = tokens[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Convert to DynamicCache\n",
    "    cache = DynamicCache()\n",
    "    for layer_idx, (k, v) in enumerate(cache_list):\n",
    "        cache.update(k, v, layer_idx)\n",
    "    \n",
    "    cache_len = cache.get_seq_length()\n",
    "    \n",
    "    # Process last token with cached context\n",
    "    last_token = tokens[\"input_ids\"][:, -1:]\n",
    "    position_ids = torch.tensor([[cache_len - 1]], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=last_token,\n",
    "            past_key_values=cache,\n",
    "            position_ids=position_ids,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Sample first token\n",
    "    logits = out.logits[0, -1, :] / temperature\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated.append(next_token.item())\n",
    "    \n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Continue generation\n",
    "    cache = out.past_key_values\n",
    "    input_ids = next_token.unsqueeze(0)\n",
    "    current_position = cache_len\n",
    "    \n",
    "    for _ in range(1, max_new):\n",
    "        position_ids = torch.tensor([[current_position]], device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=cache,\n",
    "                position_ids=position_ids,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        logits = out.logits[0, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        input_ids = next_token.unsqueeze(0)\n",
    "        cache = out.past_key_values\n",
    "        current_position += 1\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    device = \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\").to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "    \n",
    "    prompts = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Can you tell me a joke about cats.\",\n",
    "        \"Do you know anything about India?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROMPT PACKING WITH KV CACHE EXTRACTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Pack prompts\n",
    "    print(\"\\nStep 1: Pack prompts into single sequence\")\n",
    "    packed_input_ids, prompt_ranges, prompt_lengths = pack_prompts(prompts, tokenizer, device)\n",
    "    seq_len = packed_input_ids.shape[1]\n",
    "    \n",
    "    print(f\"  Packed sequence length: {seq_len}\")\n",
    "    print(f\"  Prompt ranges: {prompt_ranges}\")\n",
    "    for i, (prompt, length) in enumerate(zip(prompts, prompt_lengths)):\n",
    "        print(f\"    Prompt {i+1}: '{prompt}' -> {length} tokens\")\n",
    "    \n",
    "    # Step 2: Create block-diagonal attention mask\n",
    "    print(\"\\nStep 2: Build block-diagonal attention mask\")\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    attn_mask = create_block_diagonal_mask(seq_len, prompt_ranges, num_heads, device)\n",
    "    print(f\"  Attention mask shape: {attn_mask.shape}\")\n",
    "    \n",
    "    # Step 3: Single forward pass with packed prompts\n",
    "    print(\"\\nStep 3: Forward pass with prompt packing\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=packed_input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    print(\"  ✓ Forward pass complete\")\n",
    "    \n",
    "    # Step 4: Extract individual KV caches\n",
    "    print(\"\\nStep 4: Extract KV caches for each prompt\")\n",
    "    all_caches = extract_prompt_caches(outputs.past_key_values, prompt_ranges)\n",
    "    for i, (start, end) in enumerate(prompt_ranges):\n",
    "        print(f\"  Prompt {i+1}: Extracted cache with {end-start} tokens\")\n",
    "    \n",
    "    # Step 5: Generate from each cache\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 5: Generate from extracted caches\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, (prompt, cache) in enumerate(zip(prompts, all_caches)):\n",
    "        print(f\"\\nPrompt {i+1}: '{prompt}'\")\n",
    "        result = generate_from_cache(\n",
    "            prompt_text=prompt,\n",
    "            cache_list=cache,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            max_new=50,\n",
    "            temperature=0.8,\n",
    "            top_k=50\n",
    "        )\n",
    "        print(f\"Generated: {result}\\n\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"✓ Prompt packing with KV cache extraction complete!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KV CACHE DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "This demo shows how KV caching optimizes overlapping prompts\n",
      "by reusing computed Key-Value pairs from previous tokens.\n",
      "\n",
      "\n",
      "Prompt 1: What is Machine Learning?\n",
      "Prompt 2: What is Machine Learning and Deep Learning?\n",
      "\n",
      "Shared tokens: ['What', 'is', 'Machine', 'Learning?']\n",
      "Shared prefix length: 4 tokens\n",
      "\n",
      "============================================================\n",
      "PROMPT 1: 'What is Machine Learning?'\n",
      "============================================================\n",
      "Tokens: ['What', 'is', 'Machine', 'Learning?']\n",
      "Number of tokens: 4\n",
      "Embedding shape: (4, 64)\n",
      "  ✓ Computing all 4 tokens (no cache)\n",
      "\n",
      "Computation Stats:\n",
      "  - Total tokens: 4\n",
      "  - Tokens from cache: 0\n",
      "  - Tokens computed: 4\n",
      "  - Time elapsed: 12.49ms\n",
      "\n",
      "Output shape: (4, 64)\n",
      "Cache contains: 4 token K/V pairs\n",
      "\n",
      "============================================================\n",
      "BASELINE: Processing Prompt 2 WITHOUT cache\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROMPT 2 (No Cache): 'What is Machine Learning and Deep Learning?'\n",
      "============================================================\n",
      "Tokens: ['What', 'is', 'Machine', 'Learning', 'and', 'Deep', 'Learning?']\n",
      "Number of tokens: 7\n",
      "Embedding shape: (7, 64)\n",
      "  ✓ Computing all 7 tokens (no cache)\n",
      "\n",
      "Computation Stats:\n",
      "  - Total tokens: 7\n",
      "  - Tokens from cache: 0\n",
      "  - Tokens computed: 7\n",
      "  - Time elapsed: 1.05ms\n",
      "\n",
      "Output shape: (7, 64)\n",
      "Cache contains: 7 token K/V pairs\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED: Processing Prompt 2 WITH cache from Prompt 1\n",
      "============================================================\n",
      "New tokens only: ['Learning', 'and', 'Deep']\n",
      "\n",
      "============================================================\n",
      "PROMPT 2 (With Cache): 'Learning and Deep'\n",
      "============================================================\n",
      "Tokens: ['Learning', 'and', 'Deep']\n",
      "Number of tokens: 3\n",
      "Embedding shape: (3, 64)\n",
      "  ✓ Reusing 4 tokens from cache\n",
      "  ✓ Computing 3 new tokens\n",
      "\n",
      "Computation Stats:\n",
      "  - Total tokens: 3\n",
      "  - Tokens from cache: 4\n",
      "  - Tokens computed: 3\n",
      "  - Cache savings: 133.3%\n",
      "  - Time elapsed: 0.78ms\n",
      "\n",
      "Output shape: (3, 64)\n",
      "Cache contains: 7 token K/V pairs\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Prompt 2 without cache:\n",
      "  - Tokens computed: 7\n",
      "\n",
      "Prompt 2 with cache:\n",
      "  - Tokens from cache: 4\n",
      "  - Tokens computed: 3\n",
      "  - Total savings: 57.1%\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHTS\n",
      "============================================================\n",
      "\n",
      "1. Without cache: Must compute K,V for ALL tokens in Prompt 2\n",
      "2. With cache: Reuse K,V for shared prefix, only compute new tokens\n",
      "3. Memory cost: Store cached K,V pairs (increases linearly)\n",
      "4. Compute savings: Proportional to shared prefix length\n",
      "5. Use cases: Chat history, auto-completion, beam search\n",
      "    \n",
      "\n",
      "============================================================\n",
      "CACHE STRUCTURE\n",
      "============================================================\n",
      "\n",
      "Cache after Prompt 1:\n",
      "  - Keys shape: (4, 64)\n",
      "  - Values shape: (4, 64)\n",
      "  - Represents 4 cached tokens\n",
      "\n",
      "Cache after Prompt 2:\n",
      "  - Keys shape: (7, 64)\n",
      "  - Values shape: (7, 64)\n",
      "  - Represents 7 cached tokens\n",
      "  - Growth: +3 tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "\n",
    "class SimpleAttention:\n",
    "    \"\"\"Simplified attention mechanism with KV caching\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=64, n_heads=4):\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        np.random.seed(42)\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.01\n",
    "        \n",
    "    def compute_attention(self, Q, K, V):\n",
    "        \"\"\"Compute scaled dot-product attention\"\"\"\n",
    "        scores = np.matmul(Q, K.T) / np.sqrt(self.d_k)\n",
    "        attention_weights = self._softmax(scores)\n",
    "        output = np.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        \"\"\"\n",
    "        Forward pass with optional KV caching\n",
    "        \n",
    "        Args:\n",
    "            x: Input embeddings [seq_len, d_model]\n",
    "            kv_cache: Dict with 'keys' and 'values' or None\n",
    "            \n",
    "        Returns:\n",
    "            output, new_kv_cache, computation_stats\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = np.matmul(x, self.W_q)\n",
    "        K_new = np.matmul(x, self.W_k)\n",
    "        V_new = np.matmul(x, self.W_v)\n",
    "        \n",
    "        stats = {\n",
    "            'tokens_processed': seq_len,\n",
    "            'tokens_computed': seq_len,\n",
    "            'tokens_from_cache': 0\n",
    "        }\n",
    "        \n",
    "        # Handle KV cache\n",
    "        if kv_cache is not None and 'keys' in kv_cache:\n",
    "            # Concatenate cached K, V with new ones\n",
    "            K = np.concatenate([kv_cache['keys'], K_new], axis=0)\n",
    "            V = np.concatenate([kv_cache['values'], V_new], axis=0)\n",
    "            \n",
    "            stats['tokens_from_cache'] = kv_cache['keys'].shape[0]\n",
    "            stats['tokens_computed'] = seq_len\n",
    "            \n",
    "            print(f\"  ✓ Reusing {stats['tokens_from_cache']} tokens from cache\")\n",
    "            print(f\"  ✓ Computing {stats['tokens_computed']} new tokens\")\n",
    "        else:\n",
    "            K = K_new\n",
    "            V = V_new\n",
    "            print(f\"  ✓ Computing all {seq_len} tokens (no cache)\")\n",
    "        \n",
    "        # Compute attention\n",
    "        output, attention_weights = self.compute_attention(Q, K, V)\n",
    "        \n",
    "        # Project output\n",
    "        output = np.matmul(output, self.W_o)\n",
    "        \n",
    "        # Create new cache with ALL K, V (including new ones)\n",
    "        new_cache = {\n",
    "            'keys': K,\n",
    "            'values': V\n",
    "        }\n",
    "        \n",
    "        return output, new_cache, stats\n",
    "\n",
    "\n",
    "class KVCacheDemo:\n",
    "    \"\"\"Demonstration of KV caching with two prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=64):\n",
    "        self.d_model = d_model\n",
    "        self.attention = SimpleAttention(d_model=d_model)\n",
    "        self.vocab = {}\n",
    "        self.embeddings = {}\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization by splitting on spaces\"\"\"\n",
    "        return text.split()\n",
    "    \n",
    "    def get_embedding(self, token: str) -> np.ndarray:\n",
    "        \"\"\"Get or create embedding for a token\"\"\"\n",
    "        if token not in self.embeddings:\n",
    "            # Create random embedding for new token\n",
    "            self.embeddings[token] = np.random.randn(self.d_model) * 0.1\n",
    "        return self.embeddings[token]\n",
    "    \n",
    "    def encode(self, tokens: List[str]) -> np.ndarray:\n",
    "        \"\"\"Convert tokens to embeddings\"\"\"\n",
    "        return np.array([self.get_embedding(token) for token in tokens])\n",
    "    \n",
    "    def process_prompt(self, prompt: str, kv_cache: Optional[Dict] = None, \n",
    "                      prompt_name: str = \"Prompt\") -> Tuple[np.ndarray, Dict, Dict]:\n",
    "        \"\"\"Process a prompt with optional KV cache\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{prompt_name}: '{prompt}'\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize(prompt)\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Number of tokens: {len(tokens)}\")\n",
    "        \n",
    "        # Encode\n",
    "        x = self.encode(tokens)\n",
    "        print(f\"Embedding shape: {x.shape}\")\n",
    "        \n",
    "        # Forward pass with timing\n",
    "        start_time = time.time()\n",
    "        output, new_cache, stats = self.attention.forward(x, kv_cache)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nComputation Stats:\")\n",
    "        print(f\"  - Total tokens: {stats['tokens_processed']}\")\n",
    "        print(f\"  - Tokens from cache: {stats['tokens_from_cache']}\")\n",
    "        print(f\"  - Tokens computed: {stats['tokens_computed']}\")\n",
    "        if stats['tokens_from_cache'] > 0:\n",
    "            savings = (stats['tokens_from_cache'] / stats['tokens_processed']) * 100\n",
    "            print(f\"  - Cache savings: {savings:.1f}%\")\n",
    "        print(f\"  - Time elapsed: {elapsed*1000:.2f}ms\")\n",
    "        print(f\"\\nOutput shape: {output.shape}\")\n",
    "        print(f\"Cache contains: {new_cache['keys'].shape[0]} token K/V pairs\")\n",
    "        \n",
    "        return output, new_cache, stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main demonstration\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"KV CACHE DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThis demo shows how KV caching optimizes overlapping prompts\")\n",
    "    print(\"by reusing computed Key-Value pairs from previous tokens.\\n\")\n",
    "    \n",
    "    # Initialize demo\n",
    "    demo = KVCacheDemo(d_model=64)\n",
    "    \n",
    "    # Define prompts\n",
    "    prompt1 = \"What is Machine Learning?\"\n",
    "    prompt2 = \"What is Machine Learning and Deep Learning?\"\n",
    "    \n",
    "    print(f\"\\nPrompt 1: {prompt1}\")\n",
    "    print(f\"Prompt 2: {prompt2}\")\n",
    "    \n",
    "    # Identify shared prefix\n",
    "    tokens1 = demo.tokenize(prompt1)\n",
    "    tokens2 = demo.tokenize(prompt2)\n",
    "    shared_tokens = [t for t in tokens1 if t in tokens2]\n",
    "    print(f\"\\nShared tokens: {shared_tokens}\")\n",
    "    print(f\"Shared prefix length: {len(shared_tokens)} tokens\")\n",
    "    \n",
    "    # Process Prompt 1 (no cache)\n",
    "    output1, cache1, stats1 = demo.process_prompt(prompt1, kv_cache=None, \n",
    "                                                   prompt_name=\"PROMPT 1\")\n",
    "    \n",
    "    # Process Prompt 2 WITHOUT cache (for comparison)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE: Processing Prompt 2 WITHOUT cache\")\n",
    "    print(\"=\"*60)\n",
    "    output2_no_cache, _, stats2_no_cache = demo.process_prompt(\n",
    "        prompt2, kv_cache=None, prompt_name=\"PROMPT 2 (No Cache)\"\n",
    "    )\n",
    "    \n",
    "    # Process Prompt 2 WITH cache from Prompt 1\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMIZED: Processing Prompt 2 WITH cache from Prompt 1\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract only the NEW tokens for Prompt 2\n",
    "    new_tokens = [t for t in tokens2 if t not in tokens1]\n",
    "    new_prompt = \" \".join(new_tokens)\n",
    "    \n",
    "    print(f\"New tokens only: {new_tokens}\")\n",
    "    output2_cached, cache2, stats2_cached = demo.process_prompt(\n",
    "        new_prompt, kv_cache=cache1, prompt_name=\"PROMPT 2 (With Cache)\"\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPrompt 2 without cache:\")\n",
    "    print(f\"  - Tokens computed: {stats2_no_cache['tokens_computed']}\")\n",
    "    print(f\"\\nPrompt 2 with cache:\")\n",
    "    print(f\"  - Tokens from cache: {stats2_cached['tokens_from_cache']}\")\n",
    "    print(f\"  - Tokens computed: {stats2_cached['tokens_computed']}\")\n",
    "    total_tokens = stats2_cached['tokens_from_cache'] + stats2_cached['tokens_computed']\n",
    "    savings = (stats2_cached['tokens_from_cache'] / total_tokens) * 100\n",
    "    print(f\"  - Total savings: {savings:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "1. Without cache: Must compute K,V for ALL tokens in Prompt 2\n",
    "2. With cache: Reuse K,V for shared prefix, only compute new tokens\n",
    "3. Memory cost: Store cached K,V pairs (increases linearly)\n",
    "4. Compute savings: Proportional to shared prefix length\n",
    "5. Use cases: Chat history, auto-completion, beam search\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CACHE STRUCTURE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nCache after Prompt 1:\")\n",
    "    print(f\"  - Keys shape: {cache1['keys'].shape}\")\n",
    "    print(f\"  - Values shape: {cache1['values'].shape}\")\n",
    "    print(f\"  - Represents {cache1['keys'].shape[0]} cached tokens\")\n",
    "    \n",
    "    print(f\"\\nCache after Prompt 2:\")\n",
    "    print(f\"  - Keys shape: {cache2['keys'].shape}\")\n",
    "    print(f\"  - Values shape: {cache2['values'].shape}\")\n",
    "    print(f\"  - Represents {cache2['keys'].shape[0]} cached tokens\")\n",
    "    print(f\"  - Growth: +{cache2['keys'].shape[0] - cache1['keys'].shape[0]} tokens\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
