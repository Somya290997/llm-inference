{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2949d3-e970-42c2-8eb8-7bb1139a6e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/root/miniconda/envs/likhitenv/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/root/miniconda/envs/likhitenv/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'decode_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/root/miniconda/envs/likhitenv/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/root/miniconda/envs/likhitenv/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'prefill_worker' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# gpu_kv_cache_manager.py\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from queue import Empty\n",
    "import time\n",
    "\n",
    "# Model config\n",
    "n_layers = 2  # keep small for example\n",
    "n_heads = 4\n",
    "head_dim = 32\n",
    "max_seq = 128\n",
    "batch_slots = 4  # how many concurrent requests you support\n",
    "\n",
    "def allocate_kv_cache(slot_id):\n",
    "    kv_layout = {}\n",
    "    for layer in range(n_layers):\n",
    "        k_tensor = torch.empty((n_heads, max_seq, head_dim), dtype=torch.float16, device='cuda:1')\n",
    "        v_tensor = torch.empty_like(k_tensor)\n",
    "        kv_layout[layer] = {\"K\": k_tensor, \"V\": v_tensor}\n",
    "    return kv_layout\n",
    "\n",
    "def decode_worker(decode_to_prefill_q, prefill_to_decode_q, page_table):\n",
    "    torch.cuda.set_device(1)  # Decode GPU\n",
    "    print(\"[Decode] Waiting for requests...\")\n",
    "\n",
    "    request_id = \"req1\"\n",
    "    prompt = \"What is machine learning?\"\n",
    "    slot_id = 0\n",
    "\n",
    "    # Step 1: Allocate KV cache and build page table\n",
    "    kv_layout = allocate_kv_cache(slot_id)\n",
    "    page_table[request_id] = {\n",
    "        \"slot_id\": slot_id,\n",
    "        \"ready_layers\": 0,\n",
    "        \"kv_layout\": kv_layout,\n",
    "        \"max_seq\": max_seq\n",
    "    }\n",
    "\n",
    "    # Step 2: Send prompt + kv layout metadata to prefill\n",
    "    decode_to_prefill_q.put({\"request_id\": request_id, \"prompt\": prompt})\n",
    "\n",
    "    # Step 3: Wait for all layers to be ready\n",
    "    while page_table[request_id][\"ready_layers\"] < n_layers:\n",
    "        try:\n",
    "            msg = prefill_to_decode_q.get(timeout=1.0)\n",
    "            if msg.get(\"request_id\") == request_id:\n",
    "                page_table[request_id][\"ready_layers\"] += 1\n",
    "                print(f\"[Decode] Layer {page_table[request_id]['ready_layers']} ready\")\n",
    "        except Empty:\n",
    "            pass\n",
    "\n",
    "    print(\"[Decode] All layers ready. Start decoding...\")\n",
    "    # Decode logic goes here (e.g., generate loop using KV)\n",
    "\n",
    "\n",
    "def prefill_worker(decode_to_prefill_q, prefill_to_decode_q, page_table):\n",
    "    torch.cuda.set_device(0)  # Prefill GPU\n",
    "    print(\"[Prefill] Ready to receive prompts...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            msg = decode_to_prefill_q.get(timeout=5.0)\n",
    "            request_id = msg[\"request_id\"]\n",
    "            prompt = msg[\"prompt\"]\n",
    "            print(f\"[Prefill] Received prompt: {prompt}\")\n",
    "\n",
    "            # Simulate layer-by-layer KV computation + copy\n",
    "            for layer in range(n_layers):\n",
    "                time.sleep(0.5)  # Simulate compute time\n",
    "                # Fake K/V tensors on cuda:0\n",
    "                k = torch.randn(n_heads, max_seq, head_dim, dtype=torch.float16, device='cuda:0')\n",
    "                v = torch.randn_like(k)\n",
    "\n",
    "                # Copy to pre-allocated tensor on cuda:1\n",
    "                dest_k = page_table[request_id][\"kv_layout\"][layer][\"K\"]\n",
    "                dest_v = page_table[request_id][\"kv_layout\"][layer][\"V\"]\n",
    "                dest_k.copy_(k.to('cuda:1'))\n",
    "                dest_v.copy_(v.to('cuda:1'))\n",
    "\n",
    "                prefill_to_decode_q.put({\"request_id\": request_id, \"layer\": layer})\n",
    "\n",
    "        except Empty:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "# mp.set_start_method('spawn')  # required for CUDA tensors\n",
    "\n",
    "manager = mp.Manager()\n",
    "page_table = manager.dict()\n",
    "\n",
    "decode_to_prefill_q = mp.Queue()\n",
    "prefill_to_decode_q = mp.Queue()\n",
    "\n",
    "decode_proc = mp.Process(target=decode_worker, args=(decode_to_prefill_q, prefill_to_decode_q, page_table))\n",
    "prefill_proc = mp.Process(target=prefill_worker, args=(decode_to_prefill_q, prefill_to_decode_q, page_table))\n",
    "\n",
    "decode_proc.start()\n",
    "prefill_proc.start()\n",
    "\n",
    "decode_proc.join()\n",
    "prefill_proc.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca53af-71c2-4f90-b86f-00a69d5670a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (likhitenv)",
   "language": "python",
   "name": "likhitenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
