{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likhit/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input_ids torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"This paper tackles the inefficiency that are present in current LLM serving systems that run the\n",
    "prefill first-token generation and decoding (subsequent-token generation) phases on the same GPUs. This\n",
    "causes underutilised resource and higher latency issue. The authors propose DistServe, a system that\n",
    "disaggregates prefill and decoding onto separate GPUs, allowing each phase to be optimized independently. It\n",
    "also introduces an algorithm that adapts GPU allocation and parallelism to maximize the number of requests that is\n",
    "served within latency SLOs per GPU.The experiments on models can you summarise this for me please and also be short and clear ?\"\"\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(f\"The shape of input_ids {(input['input_ids'].shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the prefill block 0\n",
      "This is the prefill block 1\n",
      "This is the prefill block 2\n",
      "This is the prefill block 3\n",
      "This is the prefill block 4\n",
      "This is the prefill block 5\n",
      "This is the prefill block 6\n",
      "This is the prefill block 7\n",
      "This is the prefill block 8\n",
      "This is the prefill block 9\n",
      "This is the decode block and the values are [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "a = []\n",
    "\n",
    "async def prefill(a):\n",
    "    for i in range(0,10):\n",
    "            a.append(i)\n",
    "            print(f\"This is the prefill block {i}\")\n",
    "\n",
    "async def decode(a):\n",
    "    if len(a) >= 1:\n",
    "        print(f\"This is the decode block and the values are {a}\")\n",
    "\n",
    "async def main():\n",
    "    await prefill(a)\n",
    "    await decode(a)\n",
    "\n",
    "await prefill(a)\n",
    "await decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
