{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "# from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# class SimpleKVCache:\n",
    "#     \"\"\"Simple KV Cache store with prefix matching for LLM inference\"\"\"\n",
    "    \n",
    "#     def __init__(self, model, tokenizer):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.device = next(model.parameters()).device\n",
    "        \n",
    "#         # KV Store: maps token_ids tuple -> DynamicCache\n",
    "#         self.kv_store: Dict[tuple, DynamicCache] = {}\n",
    "    \n",
    "#     def clone_cache(self, cache: DynamicCache) -> DynamicCache:\n",
    "#         \"\"\"Deep copy a cache\"\"\"\n",
    "#         new_cache = DynamicCache()\n",
    "#         for i in range(len(cache)):\n",
    "#             k, v = cache[i]\n",
    "#             new_cache.update(k.clone(), v.clone(), i)\n",
    "#         return new_cache\n",
    "    \n",
    "#     def slice_cache(self, cache: DynamicCache, length: int) -> DynamicCache:\n",
    "#         \"\"\"Slice cache to specific length\"\"\"\n",
    "#         new_cache = DynamicCache()\n",
    "#         for i in range(len(cache)):\n",
    "#             k, v = cache[i]\n",
    "#             new_cache.update(k[:,:,:length,:].clone(), v[:,:,:length,:].clone(), i)\n",
    "#         return new_cache\n",
    "    \n",
    "#     def find_longest_prefix(self, token_ids: List[int]) -> Tuple[Optional[DynamicCache], int]:\n",
    "#         \"\"\"Find longest matching prefix in KV store\"\"\"\n",
    "#         best_cache = None\n",
    "#         best_len = 0\n",
    "        \n",
    "#         for stored_ids, stored_cache in self.kv_store.items():\n",
    "#             # Find matching length\n",
    "#             match_len = 0\n",
    "#             for i in range(min(len(token_ids), len(stored_ids))):\n",
    "#                 if token_ids[i] == stored_ids[i]:\n",
    "#                     match_len += 1\n",
    "#                 else:\n",
    "#                     break\n",
    "            \n",
    "#             # Update best match (require at least 2 tokens to avoid just <bos>)\n",
    "#             if match_len > best_len and match_len >= 2:\n",
    "#                 best_len = match_len\n",
    "#                 best_cache = stored_cache\n",
    "        \n",
    "#         if best_cache:\n",
    "#             return self.slice_cache(best_cache, best_len), best_len\n",
    "#         return None, 0\n",
    "    \n",
    "#     def prefill(self, token_ids: List[int], past_kv: Optional[DynamicCache] = None) -> DynamicCache:\n",
    "#         \"\"\"Run prefill and return KV cache\"\"\"\n",
    "#         input_ids = torch.tensor([token_ids], dtype=torch.long).to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             out = self.model(input_ids=input_ids, past_key_values=past_kv, use_cache=True)\n",
    "#         return out.past_key_values\n",
    "    \n",
    "#     def decode(self, token_id: int, past_kv: DynamicCache) -> Tuple[int, DynamicCache]:\n",
    "#         \"\"\"Single decode step - returns next token and updated cache\"\"\"\n",
    "#         input_ids = torch.tensor([[token_id]], dtype=torch.long).to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             out = self.model(input_ids=input_ids, past_key_values=past_kv, use_cache=True)\n",
    "#         next_token = torch.argmax(out.logits[:, -1, :], dim=-1).item()\n",
    "#         return next_token, out.past_key_values\n",
    "    \n",
    "#     def generate(self, prompt: str, max_new_tokens: int = 50) -> str:\n",
    "#         \"\"\"\n",
    "#         Main generation function:\n",
    "#         1. Tokenize prompt\n",
    "#         2. Check KV store for prefix match\n",
    "#         3. Prefill (full or partial)\n",
    "#         4. Store KV in store\n",
    "#         5. Decode to generate tokens\n",
    "#         \"\"\"\n",
    "#         # Step 1: Tokenize\n",
    "#         token_ids = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Prompt: '{prompt}'\")\n",
    "#         print(f\"Tokens: {token_ids} ({len(token_ids)} tokens)\")\n",
    "        \n",
    "#         # Step 2: Check KV store for prefix match\n",
    "#         cached_kv, prefix_len = self.find_longest_prefix(token_ids)\n",
    "        \n",
    "#         # Step 3: Prefill\n",
    "#         if cached_kv and prefix_len > 0:\n",
    "#             # Partial prefill - only compute remaining tokens\n",
    "#             remaining = token_ids[prefix_len:]\n",
    "#             print(f\"✓ Cache HIT: {prefix_len} tokens cached, computing {len(remaining)} new tokens\")\n",
    "#             kv_cache = self.prefill(remaining, past_kv=cached_kv)\n",
    "#         else:\n",
    "#             # Full prefill - compute all tokens\n",
    "#             print(f\"✗ Cache MISS: computing all {len(token_ids)} tokens\")\n",
    "#             kv_cache = self.prefill(token_ids)\n",
    "        \n",
    "#         # Step 4: Store in KV store\n",
    "#         self.kv_store[tuple(token_ids)] = self.clone_cache(kv_cache)\n",
    "#         print(f\"  Stored in KV store (total entries: {len(self.kv_store)})\")\n",
    "        \n",
    "#         # Step 5: Decode - generate new tokens\n",
    "#         print(f\"  Generating up to {max_new_tokens} tokens...\")\n",
    "#         generated = []\n",
    "#         current_token = token_ids[-1]\n",
    "        \n",
    "#         for _ in range(max_new_tokens):\n",
    "#             next_token, kv_cache = self.decode(current_token, kv_cache)\n",
    "#             if next_token == self.tokenizer.eos_token_id:\n",
    "#                 break\n",
    "#             generated.append(next_token)\n",
    "#             current_token = next_token\n",
    "        \n",
    "#         # Decode output\n",
    "#         output_text = self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "#         full_response = prompt + output_text\n",
    "        \n",
    "#         print(f\"{'='*60}\")\n",
    "#         return full_response\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # MAIN: Simple demonstration\n",
    "# # ============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load model\n",
    "#     print(\"Loading model...\")\n",
    "#     MODEL = \"google/gemma-3-1b-it\"\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.float16, device_map=\"cpu\")\n",
    "    \n",
    "#     # Create KV cache manager\n",
    "#     kv_cache = SimpleKVCache(model, tokenizer)\n",
    "    \n",
    "#     # --------------------------------------------------------\n",
    "#     # PROMPT 1: First query (no cache)\n",
    "#     # --------------------------------------------------------\n",
    "#     prompt1 = \"What is machine learning\"\n",
    "#     response1 = kv_cache.generate(prompt1, max_new_tokens=70)\n",
    "#     print(f\"\\nResponse 1:\\n{response1}\\n\")\n",
    "    \n",
    "#     # --------------------------------------------------------\n",
    "#     # PROMPT 2: Query with prefix match (reuses \"What is machine learning\")\n",
    "#     # --------------------------------------------------------\n",
    "#     prompt2 = \"What is machine learning formula are maths ?\"\n",
    "#     response2 = kv_cache.generate(prompt2, max_new_tokens=70)\n",
    "#     print(f\"\\nResponse 2:\\n{response2}\\n\")\n",
    "    \n",
    "#     # --------------------------------------------------------\n",
    "#     # PROMPT 3: Another prefix match\n",
    "#     # --------------------------------------------------------\n",
    "#     prompt3 = \"What is machine learning used for\"\n",
    "#     response3 = kv_cache.generate(prompt3, max_new_tokens=70)\n",
    "#     print(f\"\\nResponse 3:\\n{response3}\\n\")\n",
    "    \n",
    "#     # --------------------------------------------------------\n",
    "#     # Show KV Store contents\n",
    "#     # --------------------------------------------------------\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"KV STORE CONTENTS:\")\n",
    "#     print(\"=\"*60)\n",
    "#     for i, (ids, cache) in enumerate(kv_cache.kv_store.items(), 1):\n",
    "#         text = tokenizer.decode(list(ids), skip_special_tokens=True)\n",
    "#         seq_len = cache.get_seq_length()\n",
    "#         print(f\"{i}. '{text}' -> {seq_len} cached positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "\n",
      "======================================================================\n",
      "BATCH 1: Initial prompts (no cache)\n",
      "======================================================================\n",
      "======================================================================\n",
      "BATCH PROCESSING WITH PREFIX MATCHING + PACKING\n",
      "======================================================================\n",
      "\n",
      "Step 1: Tokenize & check KV store for prefixes\n",
      "  [0] 'What is machine learning...' -> ✗ MISS, 5 to compute\n",
      "  [1] 'Explain neural networks...' -> ✗ MISS, 4 to compute\n",
      "  [2] 'Tell me about Python...' -> ✗ MISS, 5 to compute\n",
      "\n",
      "Step 2: Pack remaining tokens\n",
      "  Packed sequence length: 16 tokens\n",
      "  Ranges: [(0, 5), (6, 10), (11, 16)]\n",
      "\n",
      "Step 3: Build attention mask\n",
      "  Mask shape: torch.Size([1, 4, 16, 16])\n",
      "\n",
      "Step 4: Forward pass (packed)\n",
      "  ✓ Computed KV for 16 packed tokens\n",
      "\n",
      "Step 5: Extract & combine KV caches\n",
      "  [0] Extracted: 5 tokens\n",
      "  [1] Extracted: 4 tokens\n",
      "  [2] Extracted: 5 tokens\n",
      "\n",
      "  KV Store now has 3 entries\n",
      "\n",
      "Generating responses:\n",
      "  'What is machine learning...' -> ?\n",
      "\n",
      "Machine learning is a branch of artificial intelligence (AI) that focuses on enabling computers to learn from data without being explicitly programmed. Instead of providing step-by-step instructions, machine learning algorithms analyze data, identify patterns, and make predictions or decisions based on those patterns.\n",
      "\n",
      "Key Concepts:\n",
      "\n",
      "*   **Data:** The raw material used to train a machine learning model.\n",
      "*   **Algorithms:** The sets of rules or procedures that the machine learning model follows to learn from data.\n",
      "*   **Training:** The process of feeding data to an algorithm so it can learn.\n",
      "...\n",
      "  'Explain neural networks...' -> \n",
      "\n",
      "A neural network is a type of machine learning model that is inspired by the structure and function of the human brain.\n",
      "\n",
      "Here's a breakdown of what it is:\n",
      "\n",
      "1.  **Basic Components:**\n",
      "    *   **Neurons (Nodes):** These are the fundamental building blocks of a neural network. They receive input, process it, and produce an output.\n",
      "    *   **Connections (Edges):** Neurons are connected to each other. Each connection has a weight associated with it. These weights determine the strength of the connection.\n",
      "    *   **Layers:** Neur...\n",
      "  'Tell me about Python...' -> .\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "print(factorial(5))\n",
      "```\n",
      "\n",
      "This code calculates the factorial of a number using recursion.\n",
      "\n",
      "Let's break down the code:\n",
      "\n",
      "1.  `def factorial(n):` defines a function named `factorial` that takes one argument, `n`.\n",
      "2.  `if n == 0:`: This is the base case of the recursion. If `n` is...\n",
      "\n",
      "======================================================================\n",
      "BATCH 2: Prompts with prefix matches\n",
      "======================================================================\n",
      "======================================================================\n",
      "BATCH PROCESSING WITH PREFIX MATCHING + PACKING\n",
      "======================================================================\n",
      "\n",
      "Step 1: Tokenize & check KV store for prefixes\n",
      "  [0] 'What is machine learning and deep learni...' -> ✓ HIT (5 cached), 3 to compute\n",
      "  [1] 'Explain neural networks in simple terms...' -> ✓ HIT (4 cached), 3 to compute\n",
      "  [2] 'What is machine learning used for...' -> ✓ HIT (5 cached), 2 to compute\n",
      "  [3] 'Can you write me something about India?...' -> ✗ MISS, 9 to compute\n",
      "\n",
      "Step 2: Pack remaining tokens\n",
      "  Packed sequence length: 20 tokens\n",
      "  Ranges: [(0, 3), (4, 7), (8, 10), (11, 20)]\n",
      "\n",
      "Step 3: Build attention mask\n",
      "  Mask shape: torch.Size([1, 4, 20, 20])\n",
      "\n",
      "Step 4: Forward pass (packed)\n",
      "  ✓ Computed KV for 20 packed tokens\n",
      "\n",
      "Step 5: Extract & combine KV caches\n",
      "  [0] Combined: 5 cached + 3 new = 8 total\n",
      "  [1] Combined: 4 cached + 3 new = 7 total\n",
      "  [2] Combined: 5 cached + 2 new = 7 total\n",
      "  [3] Extracted: 9 tokens\n",
      "\n",
      "  KV Store now has 7 entries\n",
      "\n",
      "Generating responses:\n",
      "  'What is machine learning and d...' -> ?\n",
      "\n",
      "Deep learning is a subset of machine learning. It's a type of machine learning that uses artificial neural networks with multiple layers to analyze data.\n",
      "\n",
      "Here's a breakdown of key concepts:\n",
      "\n",
      "1.  **Machine Learning:**\n",
      "    *   The process of teaching computers to learn from data without being explicitly programmed.\n",
      "    *   Instead of giving a computer a set of rules, you feed it data and let it find patterns.\n",
      "\n",
      "2.  **Deep Learning:**\n",
      "    *   A specific type of machine learning that uses artificial neural networks with many layers.\n",
      "    ...\n",
      "  'Explain neural networks in sim...' -> .\n",
      "\n",
      "**What is a neural network?**\n",
      "\n",
      "A neural network is like a really smart computer program that learns from data. It's designed to recognize patterns and make predictions.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "1.  **Input:** You feed the network some information, like a picture of a cat.\n",
      "2.  **Layers:** The network is made up of layers of interconnected \"neurons\" (think of them as tiny decision-makers).\n",
      "3.  **Connections:** These neurons are connected to each other.\n",
      "4.  **Learning:** The network adjusts...\n",
      "  'What is machine learning used ...' -> ?\n",
      "\n",
      "Machine learning is a type of artificial intelligence that allows computers to learn from data without being explicitly programmed. Instead of giving a computer a set of rules, we feed it data, and it learns to identify patterns and make predictions.\n",
      "\n",
      "Here's a breakdown of what machine learning is used for:\n",
      "\n",
      "*   **Predictive Analytics:** Predicting future outcomes based on historical data.\n",
      "*   **Recommendation Systems:** Suggesting items or content based on user preferences.\n",
      "*   **Image Recognition:** Identifying objects and patterns in images.\n",
      "*   **Natural Language Processing:** Understanding and generating human...\n",
      "  'Can you write me something abo...' -> \n",
      "\n",
      "India.\n",
      "\n",
      "A land of contrasts.\n",
      "\n",
      "A land of ancient empires, of vibrant colours, of a thousand stories.\n",
      "\n",
      "It is a place where the sun rises over the Himalayas,\n",
      "a breathtaking sight.\n",
      "\n",
      "The air is thick with the scent of spices,\n",
      "a fragrant symphony.\n",
      "\n",
      "The rivers flow,\n",
      "carrying the history of the land.\n",
      "\n",
      "The people,\n",
      "a kaleidoscope of faces,\n",
      "each with a story to tell.\n",
      "\n",
      "India is a place of faith,\n",
      "a land of devotion.\n",
      "\n",
      "The temples stand tall,\n",
      "a testament to the power of the gods....\n",
      "\n",
      "======================================================================\n",
      "FINAL KV STORE:\n",
      "======================================================================\n",
      "  1. 'What is machine learning' (5 tokens)\n",
      "  2. 'Explain neural networks' (4 tokens)\n",
      "  3. 'Tell me about Python' (5 tokens)\n",
      "  4. 'What is machine learning and deep learning' (8 tokens)\n",
      "  5. 'Explain neural networks in simple terms' (7 tokens)\n",
      "  6. 'What is machine learning used for' (7 tokens)\n",
      "  7. 'Can you write me something about India?' (9 tokens)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class PackedKVCache:\n",
    "    \"\"\"\n",
    "    Combines:\n",
    "    1. KV Cache Store with prefix matching\n",
    "    2. Prompt packing for batch processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.num_heads = model.config.num_attention_heads\n",
    "        self.num_layers = model.config.num_hidden_layers\n",
    "        \n",
    "        # KV Store: token_ids (tuple) -> list of (key, value) per layer\n",
    "        self.kv_store: Dict[tuple, List[Tuple[torch.Tensor, torch.Tensor]]] = {}\n",
    "    \n",
    "    # ============================================================\n",
    "    # KV Store Operations\n",
    "    # ============================================================\n",
    "    def store_kv(self, token_ids: List[int], kv_list: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        \"\"\"Store KV cache for a token sequence\"\"\"\n",
    "        # Clone tensors to avoid mutation\n",
    "        cloned = [(k.clone(), v.clone()) for k, v in kv_list]\n",
    "        self.kv_store[tuple(token_ids)] = cloned\n",
    "    \n",
    "    def find_prefix(self, token_ids: List[int], min_match: int = 2) -> Tuple[Optional[List], int]:\n",
    "        \"\"\"Find longest prefix match in store\"\"\"\n",
    "        best_kv = None\n",
    "        best_len = 0\n",
    "        \n",
    "        for stored_ids, stored_kv in self.kv_store.items():\n",
    "            match_len = 0\n",
    "            for i in range(min(len(token_ids), len(stored_ids))):\n",
    "                if token_ids[i] == stored_ids[i]:\n",
    "                    match_len += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if match_len > best_len and match_len >= min_match:\n",
    "                best_len = match_len\n",
    "                best_kv = stored_kv\n",
    "        \n",
    "        if best_kv:\n",
    "            # Slice to prefix length\n",
    "            sliced = [(k[:,:,:best_len,:].clone(), v[:,:,:best_len,:].clone()) for k, v in best_kv]\n",
    "            return sliced, best_len\n",
    "        return None, 0\n",
    "    \n",
    "    # ============================================================\n",
    "    # Prompt Packing with Prefix Matching\n",
    "    # ============================================================\n",
    "    def process_batch(self, prompts: List[str]) -> List[Tuple[List[int], List]]:\n",
    "        \"\"\"\n",
    "        Process batch of prompts with prefix matching + packing\n",
    "        Returns: List of (token_ids, kv_cache) for each prompt\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"BATCH PROCESSING WITH PREFIX MATCHING + PACKING\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Tokenize and check prefix matches\n",
    "        print(\"\\nStep 1: Tokenize & check KV store for prefixes\")\n",
    "        \n",
    "        prompt_data = []  # (tokens, cached_kv, prefix_len, remaining_tokens)\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            tokens = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "            cached_kv, prefix_len = self.find_prefix(tokens)\n",
    "            remaining = tokens[prefix_len:] if prefix_len > 0 else tokens\n",
    "            \n",
    "            prompt_data.append({\n",
    "                'idx': i,\n",
    "                'prompt': prompt,\n",
    "                'tokens': tokens,\n",
    "                'cached_kv': cached_kv,\n",
    "                'prefix_len': prefix_len,\n",
    "                'remaining': remaining\n",
    "            })\n",
    "            \n",
    "            status = f\"✓ HIT ({prefix_len} cached)\" if prefix_len > 0 else \"✗ MISS\"\n",
    "            print(f\"  [{i}] '{prompt[:40]}...' -> {status}, {len(remaining)} to compute\")\n",
    "        \n",
    "        # Step 2: Pack remaining tokens for batch processing\n",
    "        print(\"\\nStep 2: Pack remaining tokens\")\n",
    "        \n",
    "        packed_tokens = []\n",
    "        prompt_ranges = []  # (start, end) in packed sequence\n",
    "        current_pos = 0\n",
    "        sep_token = self.tokenizer.eos_token_id\n",
    "        \n",
    "        for i, data in enumerate(prompt_data):\n",
    "            remaining = data['remaining']\n",
    "            if len(remaining) == 0:\n",
    "                prompt_ranges.append(None)  # Fully cached\n",
    "                continue\n",
    "            \n",
    "            start = current_pos\n",
    "            end = current_pos + len(remaining)\n",
    "            prompt_ranges.append((start, end))\n",
    "            \n",
    "            packed_tokens.extend(remaining)\n",
    "            \n",
    "            # Add separator between prompts\n",
    "            if i < len(prompt_data) - 1:\n",
    "                packed_tokens.append(sep_token)\n",
    "                current_pos = end + 1\n",
    "            else:\n",
    "                current_pos = end\n",
    "        \n",
    "        print(f\"  Packed sequence length: {len(packed_tokens)} tokens\")\n",
    "        print(f\"  Ranges: {prompt_ranges}\")\n",
    "        \n",
    "        # Step 3: Build block-diagonal attention mask\n",
    "        print(\"\\nStep 3: Build attention mask\")\n",
    "        \n",
    "        seq_len = len(packed_tokens)\n",
    "        if seq_len > 0:\n",
    "            attn_mask = torch.zeros((seq_len, seq_len), dtype=torch.float, device=self.device)\n",
    "            \n",
    "            for rng in prompt_ranges:\n",
    "                if rng is not None:\n",
    "                    start, end = rng\n",
    "                    attn_mask[start:end, start:end] = 1.0\n",
    "            \n",
    "            # Expand for heads: [1, heads, seq, seq]\n",
    "            attn_mask = attn_mask.unsqueeze(0).unsqueeze(0).repeat(1, self.num_heads, 1, 1)\n",
    "            print(f\"  Mask shape: {attn_mask.shape}\")\n",
    "        \n",
    "        # Step 4: Forward pass on packed sequence\n",
    "        print(\"\\nStep 4: Forward pass (packed)\")\n",
    "        \n",
    "        if seq_len > 0:\n",
    "            packed_input = torch.tensor([packed_tokens], device=self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    input_ids=packed_input,\n",
    "                    attention_mask=attn_mask,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            packed_kv = outputs.past_key_values\n",
    "            print(f\"  ✓ Computed KV for {seq_len} packed tokens\")\n",
    "        else:\n",
    "            packed_kv = None\n",
    "            print(\"  All prompts fully cached!\")\n",
    "        \n",
    "        # Step 5: Extract and combine KV caches\n",
    "        print(\"\\nStep 5: Extract & combine KV caches\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, data in enumerate(prompt_data):\n",
    "            tokens = data['tokens']\n",
    "            cached_kv = data['cached_kv']\n",
    "            prefix_len = data['prefix_len']\n",
    "            rng = prompt_ranges[i] if i < len(prompt_ranges) else None\n",
    "            \n",
    "            if rng is None and cached_kv is not None:\n",
    "                # Fully cached - use cached KV directly\n",
    "                final_kv = cached_kv\n",
    "                print(f\"  [{i}] Using full cache ({prefix_len} tokens)\")\n",
    "            \n",
    "            elif cached_kv is not None and rng is not None:\n",
    "                # Partial cache - concatenate cached + new\n",
    "                start, end = rng\n",
    "                final_kv = []\n",
    "                \n",
    "                for layer_idx in range(self.num_layers):\n",
    "                    cached_k, cached_v = cached_kv[layer_idx]\n",
    "                    new_k = packed_kv[layer_idx][0][:, :, start:end, :]\n",
    "                    new_v = packed_kv[layer_idx][1][:, :, start:end, :]\n",
    "                    \n",
    "                    combined_k = torch.cat([cached_k, new_k], dim=2)\n",
    "                    combined_v = torch.cat([cached_v, new_v], dim=2)\n",
    "                    final_kv.append((combined_k, combined_v))\n",
    "                \n",
    "                print(f\"  [{i}] Combined: {prefix_len} cached + {end-start} new = {len(tokens)} total\")\n",
    "            \n",
    "            else:\n",
    "                # No cache - extract from packed\n",
    "                start, end = rng\n",
    "                final_kv = []\n",
    "                \n",
    "                for layer_idx in range(self.num_layers):\n",
    "                    k = packed_kv[layer_idx][0][:, :, start:end, :]\n",
    "                    v = packed_kv[layer_idx][1][:, :, start:end, :]\n",
    "                    final_kv.append((k.clone(), v.clone()))\n",
    "                \n",
    "                print(f\"  [{i}] Extracted: {end-start} tokens\")\n",
    "            \n",
    "            # Store in KV store for future use\n",
    "            self.store_kv(tokens, final_kv)\n",
    "            results.append((tokens, final_kv))\n",
    "        \n",
    "        print(f\"\\n  KV Store now has {len(self.kv_store)} entries\")\n",
    "        return results\n",
    "    \n",
    "    # ============================================================\n",
    "    # Generation\n",
    "    # ============================================================\n",
    "    def generate(self, prompt: str, kv_list: List, max_new: int = 30) -> str:\n",
    "        \"\"\"Generate from a KV cache\"\"\"\n",
    "        tokens = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        \n",
    "        # Convert to DynamicCache\n",
    "        cache = DynamicCache()\n",
    "        for layer_idx, (k, v) in enumerate(kv_list):\n",
    "            cache.update(k, v, layer_idx)\n",
    "        \n",
    "        cache_len = cache.get_seq_length()\n",
    "        \n",
    "        # Start from last token\n",
    "        last_token = torch.tensor([[tokens[-1]]], device=self.device)\n",
    "        position_ids = torch.tensor([[cache_len - 1]], device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.model(\n",
    "                input_ids=last_token,\n",
    "                past_key_values=cache,\n",
    "                position_ids=position_ids,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Generate tokens\n",
    "        generated = []\n",
    "        cache = out.past_key_values\n",
    "        next_token = torch.argmax(out.logits[:, -1, :], dim=-1)\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        current_pos = cache_len\n",
    "        \n",
    "        for _ in range(max_new - 1):\n",
    "            if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            position_ids = torch.tensor([[current_pos]], device=self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = self.model(\n",
    "                    input_ids=next_token.unsqueeze(0),\n",
    "                    past_key_values=cache,\n",
    "                    position_ids=position_ids,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            cache = out.past_key_values\n",
    "            next_token = torch.argmax(out.logits[:, -1, :], dim=-1)\n",
    "            generated.append(next_token.item())\n",
    "            current_pos += 1\n",
    "        \n",
    "        return self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN DEMO\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading model...\")\n",
    "    MODEL = \"google/gemma-3-1b-it\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.float16, device_map=\"cpu\")\n",
    "    \n",
    "    cache_manager = PackedKVCache(model, tokenizer)\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # BATCH 1: First batch (no cache hits)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BATCH 1: Initial prompts (no cache)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    batch1 = [\n",
    "        \"What is machine learning\",\n",
    "        \"Explain neural networks\",\n",
    "        \"Tell me about Python\"\n",
    "    ]\n",
    "    \n",
    "    results1 = cache_manager.process_batch(batch1)\n",
    "    \n",
    "    print(\"\\nGenerating responses:\")\n",
    "    for prompt, kv in results1:\n",
    "        text = tokenizer.decode(prompt, skip_special_tokens=True)\n",
    "        response = cache_manager.generate(text, kv, max_new=120)\n",
    "        print(f\"  '{text[:30]}...' -> {response}...\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # BATCH 2: Second batch (should have prefix hits!)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BATCH 2: Prompts with prefix matches\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    batch2 = [\n",
    "        \"What is machine learning and deep learning\",  # Prefix: \"What is machine learning\"\n",
    "        \"Explain neural networks in simple terms\",      # Prefix: \"Explain neural networks\"\n",
    "        \"What is machine learning used for\",            # Prefix: \"What is machine learning\"\n",
    "        \"Can you write me something about India?\"              # Prefix: \"Tell me about Python\"\n",
    "    ]\n",
    "    \n",
    "    results2 = cache_manager.process_batch(batch2)\n",
    "    \n",
    "    print(\"\\nGenerating responses:\")\n",
    "    for prompt, kv in results2:\n",
    "        text = tokenizer.decode(prompt, skip_special_tokens=True)\n",
    "        response = cache_manager.generate(text, kv, max_new=120)\n",
    "        print(f\"  '{text[:30]}...' -> {response}...\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # Show KV Store\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL KV STORE:\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, (ids, kv) in enumerate(cache_manager.kv_store.items(), 1):\n",
    "        text = tokenizer.decode(list(ids), skip_special_tokens=True)\n",
    "        seq_len = kv[0][0].shape[2]  # Get seq length from first layer key\n",
    "        print(f\"  {i}. '{text[:50]}' ({seq_len} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9])\n"
     ]
    }
   ],
   "source": [
    "batch2 = [\n",
    "        \"What is machine learning and deep learning\",  # Prefix: \"What is machine learning\"\n",
    "        \"Explain neural networks in simple terms\",      # Prefix: \"Explain neural networks\"\n",
    "        \"What is machine learning used for\",            # Prefix: \"What is machine learning\"\n",
    "        \"Can you write me something about India?\"              # Prefix: \"Tell me about Python\"\n",
    "    ]\n",
    "\n",
    "MODEL = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "output = tokenizer(batch2, return_tensors=\"pt\" , padding=True)\n",
    "print(output[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
