# config/model_config.yaml

model_name: mistral-7b
model_path: mistralai/Mistral-7B-Instruct-v0.2 
quantization: 4bit  
compute_dtype: float16
vocab_tokens: 8
use_flash_attention: true

batch_size: 1
max_input_tokens: 512
max_output_tokens: 128
num_layers: 32
hidden_size: 4096
n_dim: 128
num_attention_heads: 8
kv_cache_dtype: float16

prefill_gpu: 1
decode_gpu: 0