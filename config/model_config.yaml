# config/model_config.yaml

model_name: mistral-7b
model_path: mistralai/Mistral-7B-Instruct-v0.2 
quantization: 4bit  
compute_dtype: float16 
use_flash_attention: true

batch_size: 2
max_input_tokens: 512
max_output_tokens: 128
num_layers: 32
hidden_size: 4096
num_attention_heads: 32
kv_cache_dtype: float16

prefill_gpu: 1
decode_gpu: 0