{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f80c5d-7f7e-4370-be3b-5a6ced1684b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735ab90741f8406395c253e2560c3197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/likhitenv/lib/python3.10/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum computing in simple terms.\n",
      "\n",
      "## Quantum Computing\n",
      "\n",
      "Quantum computing is a type of computing that uses quantum phenomena to perform operations. Quantum computing is a branch of quantum physics that uses quantum phenomena to perform operations. Quantum computing is a type of computing that uses the properties of quantum physics to perform operations. Quantum computing is a type of computing that uses quantum phenomena to perform operations. Quantum computing is a type of computing that uses quantum phenomena to perform operations. Quantum computing\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# 4bit configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load model WITHOUT accelerate, WITHOUT device_map\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=None,           # <- IMPORTANT\n",
    ")\n",
    "\n",
    "# Manually move model to GPU\n",
    "# model.to(\"cuda:0\")\n",
    "\n",
    "print(\"Model loaded on:\", next(model.parameters()).device)\n",
    "\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5ee2fc-eebc-4d06-b336-74f530221684",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ''' Can you Summarise this please ? In modern AI systems, especially those using transformer-based architectures, the size of the input prompt plays an important role in determining latency, memory usage, and the quality of generated responses. When engineers talk about a prompt being around one thousand tokens, they are usually referring to a block of text that is several paragraphs long—roughly four to five thousand characters in length. This amount of text may include system instructions, user queries, constraints, or even examples used in few-shot prompting. Understanding what this looks like in practice helps when designing systems, such as translation pipelines, retrieval-augmented generation frameworks, or multi-process inference architectures, where prompt size directly affects throughput and GPU utilization.\n",
    "\n",
    "When a transformer model receives a large prompt, it must process all tokens through every layer during the prefill stage. This is expensive, especially for decoder-only models used in LLMs. The prefill phase computes attention across all input tokens, meaning that the computational complexity increases quadratically with the prompt length. For example, a 1024-token prompt requires roughly four times the compute of a 512-token prompt for the same model. Because of this, engineers working on real-time applications, such as speech-to-text translation or conversational agents, must optimize prompt size carefully to balance context and latency.\n",
    "\n",
    "Another aspect of managing a 1024-token prompt is memory. Each token generates key and value tensors for each attention head in every layer, which are stored in the KV cache. If a model has, for example, 32 layers and 32 attention heads per layer, the KV cache for 1024 tokens can easily exceed several hundred megabytes, depending on the hidden dimension. This is why many high-performance inference frameworks, such as vLLM, FlashAttention-based servers, or custom systems using CUDA streams, focus heavily on KV cache compression, sharing, or streaming. Smaller prompt sizes significantly reduce memory footprint and allow serving more simultaneous requests on a single GPU.\n",
    "\n",
    "Another important point is prompt engineering. Even when models support long context windows, such as 32k or 128k, not all content contributes equally to the final generation quality. Effective prompting often involves rewriting or summarizing information so that the most relevant parts appear earlier in the sequence. For example, in a retrieval-augmented generation system, retrieved passages may be chunked into smaller segments so that the LLM can focus on the highest-ranking ones rather than blindly receiving large blocks of text.\n",
    "\n",
    "Finally, understanding prompt length matters when fine-tuning as well. During training, especially when using QLoRA or LoRA adapters, batching large prompt sequences increases GPU memory consumption. Many engineers limit training sequence length to 512 or 1024 tokens for efficiency, unless training a model explicitly meant for long-form reasoning. The trade-offs between sequence length, batch size, and memory often define the maximum throughput of the training loop.\n",
    "\n",
    "Overall, a 1024-token prompt is long enough to include multiple instructions, several examples, and extensive user context, but short enough to be processed efficiently by most mid-sized LLMs. Understanding its structure and impact is an essential part of building scalable, low-latency AI systems.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdae22e-838a-4583-b5c8-1715c844775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt1, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b59f3782-7a5d-45ff-ae32-8355ae76a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can you Summarise this please ? In modern AI systems, especially those using transformer-based architectures, the size of the input prompt plays an important role in determining latency, memory usage, and the quality of generated responses. When engineers talk about a prompt being around one thousand tokens, they are usually referring to a block of text that is several paragraphs long—roughly four to five thousand characters in length. This amount of text may include system instructions, user queries, constraints, or even examples used in few-shot prompting. Understanding what this looks like in practice helps when designing systems, such as translation pipelines, retrieval-augmented generation frameworks, or multi-process inference architectures, where prompt size directly affects throughput and GPU utilization.\n",
      "\n",
      "When a transformer model receives a large prompt, it must process all tokens through every layer during the prefill stage. This is expensive, especially for decoder-only models used in LLMs. The prefill phase computes attention across all input tokens, meaning that the computational complexity increases quadratically with the prompt length. For example, a 1024-token prompt requires roughly four times the compute of a 512-token prompt for the same model. Because of this, engineers working on real-time applications, such as speech-to-text translation or conversational agents, must optimize prompt size carefully to balance context and latency.\n",
      "\n",
      "Another aspect of managing a 1024-token prompt is memory. Each token generates key and value tensors for each attention head in every layer, which are stored in the KV cache. If a model has, for example, 32 layers and 32 attention heads per layer, the KV cache for 1024 tokens can easily exceed several hundred megabytes, depending on the hidden dimension. This is why many high-performance inference frameworks, such as vLLM, FlashAttention-based servers, or custom systems using CUDA streams, focus heavily on KV cache compression, sharing, or streaming. Smaller prompt sizes significantly reduce memory footprint and allow serving more simultaneous requests on a single GPU.\n",
      "\n",
      "Another important point is prompt engineering. Even when models support long context windows, such as 32k or 128k, not all content contributes equally to the final generation quality. Effective prompting often involves rewriting or summarizing information so that the most relevant parts appear earlier in the sequence. For example, in a retrieval-augmented generation system, retrieved passages may be chunked into smaller segments so that the LLM can focus on the highest-ranking ones rather than blindly receiving large blocks of text.\n",
      "\n",
      "Finally, understanding prompt length matters when fine-tuning as well. During training, especially when using QLoRA or LoRA adapters, batching large prompt sequences increases GPU memory consumption. Many engineers limit training sequence length to 512 or 1024 tokens for efficiency, unless training a model explicitly meant for long-form reasoning. The trade-offs between sequence length, batch size, and memory often define the maximum throughput of the training loop.\n",
      "\n",
      "Overall, a 1024-token prompt is long enough to include multiple instructions, several examples, and extensive user context, but short enough to be processed efficiently by most mid-sized LLMs. Understanding its structure and impact is an essential part of building scalable, low-latency AI systems.\n",
      "\n",
      "The following are the best-written answers that I could find:\n",
      "\n",
      "First Answer:\n",
      "\n",
      "A 1024-token prompt in AI is a string of around 4,000 characters that is used to provide the input for a language model. The specific tokens used depend on the language model, but they typically represent words, symbols, or other linguistic elements that convey meaning. In some cases, the prompt may be a specific sentence or a set of instructions\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30658656-31ea-4ec7-9ef2-3f17b24d2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007fbd92-6f19-44a4-89b3-722bd4504528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685b616-c527-49d9-8765-515c46e254c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.tensor([1], device=\"cuda:1\")\n",
    "# print(\"CUDA OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53815314-7fcf-4a9e-a163-fb3904b890dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1\n",
    "# !rm -rf /root/.cache/huggingface/hub/*Mistral-7B*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555ccf6-93af-48c6-8024-cca06b1c874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ~/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae65fd-1dde-4ea9-81ce-82364ae6a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29092ffb-64f7-47ce-a15b-5cdf46671562",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input IDs shape:\", inputs[\"input_ids\"].shape)\n",
    "print(\"Max position id:\", inputs[\"input_ids\"].shape[-1] - 1)\n",
    "print(\"Model max positions:\", model.config.max_position_embeddings)\n",
    "print(\"Vocab size:\", model.config.vocab_size)\n",
    "\n",
    "bad = (inputs[\"input_ids\"] >= model.config.vocab_size).any() or \\\n",
    "      (inputs[\"input_ids\"] < 0).any()\n",
    "\n",
    "print(\"Any invalid token ids?:\", bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f2d51a-a6a2-4a93-b8c5-8f7efc15f3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mistral_7b_4bit_local/tokenizer_config.json',\n",
       " './mistral_7b_4bit_local/special_tokens_map.json',\n",
       " './mistral_7b_4bit_local/tokenizer.model',\n",
       " './mistral_7b_4bit_local/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving to the model\n",
    "\n",
    "save_path = \"./mistral_7b_4bit_local\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc30e5-2cb3-4b0b-8c67-b4a53c379a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import torch\n",
    "\n",
    "# small_tensor = torch.empty([1, 32, 120, 128])      # 64 MB on GPU\n",
    "# large_tensor = torch.empty([664, 32, 120, 128])    # 1.92 GB on GPU\n",
    "\n",
    "# print(sys.getsizeof(small_tensor))   # Output: 96 bytes\n",
    "# print(sys.getsizeof(large_tensor))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde4ce5-ac3b-4ea7-a83a-9bdb106b5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# Check GPU0 → GPU1\n",
    "can_access_0_to_1 = cp.cuda.runtime.deviceCanAccessPeer(0, 1)\n",
    "print(f\"GPU0 → GPU1: {can_access_0_to_1}\")\n",
    "\n",
    "# Check GPU1 → GPU0\n",
    "can_access_1_to_0 = cp.cuda.runtime.deviceCanAccessPeer(1, 0)\n",
    "print(f\"GPU1 → GPU0: {can_access_1_to_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64239126-5fca-4e80-98b4-126cfca7f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# === Allocate a tensor on GPU1 ===\n",
    "src_tensor = torch.arange(10, dtype=torch.float32, device='cuda:1')\n",
    "src_ptr = src_tensor.data_ptr()\n",
    "size = src_tensor.numel() * src_tensor.element_size()\n",
    "\n",
    "\n",
    "# === Allocate destination on GPU0 ===\n",
    "dst_tensor = torch.empty_like(src_tensor, device='cuda:0')\n",
    "dst_ptr = dst_tensor.data_ptr()\n",
    "\n",
    "print(dst_tensor)\n",
    "print(f\"[DEBUG] src_ptr: {src_ptr}, dst_ptr: {dst_ptr}, size: {size}\")\n",
    "\n",
    "# === Copy using memcpyPeer ===\n",
    "cp.cuda.runtime.memcpyPeer(\n",
    "    dst_ptr, 0,         # destination ptr, device ID 0\n",
    "    src_ptr, 1,         # source ptr, device ID 1\n",
    "    size                # bytes to copy\n",
    ")\n",
    "\n",
    "print(dst_tensor)\n",
    "# === Check if copied correctly ===\n",
    "copied = dst_tensor.cpu().numpy()\n",
    "expected = np.arange(10, dtype=np.float32)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(\"Copied Tensor: \", copied)\n",
    "print(\"Expected:      \", expected)\n",
    "\n",
    "assert np.allclose(copied, expected), \"❌ Memory copy failed!\"\n",
    "print(\"✅ memcpyPeer between GPU1 → GPU0 succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24556bd2-11cc-48d5-8af4-c45005d80bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import torch\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def producer(q):\n",
    "    print(\"[Producer] Creating tensors...\")\n",
    "\n",
    "    src = torch.arange(10, dtype=torch.float32, device=\"cuda:1\")\n",
    "    dst = torch.zeros_like(src, device=\"cuda:0\")\n",
    "\n",
    "    message = {\n",
    "        \"src_ptr\": src.data_ptr(),\n",
    "        \"dst_ptr\": dst.data_ptr(),\n",
    "        \"size\": src.numel() * src.element_size(),\n",
    "    }\n",
    "\n",
    "    q.put(message)\n",
    "    q.put({\"dst\": dst})\n",
    "\n",
    "    time.sleep(2)  # keep alive\n",
    "\n",
    "\n",
    "def consumer(q):\n",
    "    print(\"[Consumer] Waiting for ptr info...\")\n",
    "    msg = q.get()\n",
    "    dst_info = q.get()\n",
    "\n",
    "    src_ptr = msg[\"src_ptr\"]\n",
    "    dst_ptr = msg[\"dst_ptr\"]\n",
    "    size = msg[\"size\"]\n",
    "\n",
    "    print(\"[Consumer] memcpyPeer...\")\n",
    "    cp.cuda.runtime.memcpyPeer(dst_ptr, 0, src_ptr, 1, size)\n",
    "\n",
    "    dst_tensor = dst_info[\"dst\"]\n",
    "    dst_tensor.add_(10)\n",
    "\n",
    "    print(\"[Consumer] Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    q = mp.Queue()\n",
    "\n",
    "    p1 = mp.Process(target=producer, args=(q,))\n",
    "    p2 = mp.Process(target=consumer, args=(q,))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "    print(\"✅ Main: Processes done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db151d0-7de3-427e-bd8d-dabfd881521e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
